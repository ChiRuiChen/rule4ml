{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip <command> [options]\n",
      "\n",
      "Commands:\n",
      "  install                     Install packages.\n",
      "  download                    Download packages.\n",
      "  uninstall                   Uninstall packages.\n",
      "  freeze                      Output installed packages in requirements format.\n",
      "  inspect                     Inspect the python environment.\n",
      "  list                        List installed packages.\n",
      "  show                        Show information about installed packages.\n",
      "  check                       Verify installed packages have compatible dependencies.\n",
      "  config                      Manage local and global configuration.\n",
      "  search                      Search PyPI for packages.\n",
      "  cache                       Inspect and manage pip's wheel cache.\n",
      "  index                       Inspect information available from package indexes.\n",
      "  wheel                       Build wheels from your requirements.\n",
      "  hash                        Compute hashes of package archives.\n",
      "  completion                  A helper command used for command completion.\n",
      "  debug                       Show information useful for debugging.\n",
      "  help                        Show help for commands.\n",
      "\n",
      "General Options:\n",
      "  -h, --help                  Show help.\n",
      "  --debug                     Let unhandled exceptions propagate outside the\n",
      "                              main subroutine, instead of logging them to\n",
      "                              stderr.\n",
      "  --isolated                  Run pip in an isolated mode, ignoring\n",
      "                              environment variables and user configuration.\n",
      "  --require-virtualenv        Allow pip to only run in a virtual environment;\n",
      "                              exit with an error otherwise.\n",
      "  --python <python>           Run pip with the specified Python interpreter.\n",
      "  -v, --verbose               Give more output. Option is additive, and can be\n",
      "                              used up to 3 times.\n",
      "  -V, --version               Show version and exit.\n",
      "  -q, --quiet                 Give less output. Option is additive, and can be\n",
      "                              used up to 3 times (corresponding to WARNING,\n",
      "                              ERROR, and CRITICAL logging levels).\n",
      "  --log <path>                Path to a verbose appending log.\n",
      "  --no-input                  Disable prompting for input.\n",
      "  --keyring-provider <keyring_provider>\n",
      "                              Enable the credential lookup via the keyring\n",
      "                              library if user input is allowed. Specify which\n",
      "                              mechanism to use [disabled, import, subprocess].\n",
      "                              (default: disabled)\n",
      "  --proxy <proxy>             Specify a proxy in the form\n",
      "                              scheme://[user:passwd@]proxy.server:port.\n",
      "  --retries <retries>         Maximum number of retries each connection should\n",
      "                              attempt (default 5 times).\n",
      "  --timeout <sec>             Set the socket timeout (default 15 seconds).\n",
      "  --exists-action <action>    Default action when a path already exists:\n",
      "                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n",
      "  --trusted-host <hostname>   Mark this host or host:port pair as trusted,\n",
      "                              even though it does not have valid or any HTTPS.\n",
      "  --cert <path>               Path to PEM-encoded CA certificate bundle. If\n",
      "                              provided, overrides the default. See 'SSL\n",
      "                              Certificate Verification' in pip documentation\n",
      "                              for more information.\n",
      "  --client-cert <path>        Path to SSL client certificate, a single file\n",
      "                              containing the private key and the certificate\n",
      "                              in PEM format.\n",
      "  --cache-dir <dir>           Store the cache data in <dir>.\n",
      "  --no-cache-dir              Disable the cache.\n",
      "  --disable-pip-version-check\n",
      "                              Don't periodically check PyPI to determine\n",
      "                              whether a new version of pip is available for\n",
      "                              download. Implied with --no-index.\n",
      "  --no-color                  Suppress colored output.\n",
      "  --no-python-version-warning\n",
      "                              Silence deprecation warnings for upcoming\n",
      "                              unsupported Pythons.\n",
      "  --use-feature <feature>     Enable new functionality, that may be backward\n",
      "                              incompatible.\n",
      "  --use-deprecated <feature>  Enable deprecated functionality, that will be\n",
      "                              removed in the future.\n"
     ]
    }
   ],
   "source": [
    "!pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optionally force tensorflow on CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [General Usage](#general-usage)\n",
    "    1. [Basic Usage](#basic-usage)\n",
    "    2. [Advanced Usage](#advanced-usage)\n",
    "2. [Data Generation](#data-gen)\n",
    "    1. [Model Synthesis](#model-synth)\n",
    "        1. [Keras Synthesis](#keras-synth)\n",
    "        2. [PyTorch Synthesis](#torch-synth)\n",
    "    2. [Parallel Synthesis](#parallel-synth)\n",
    "        1. [Randomly Generated Networks](#random-synth)\n",
    "3. [Training Prediction Models](#train-models)\n",
    "    1. [Parsing Datasets](#parse-data)\n",
    "        1. [Reading from JSON](#read-json)\n",
    "    2. [Training MLPs](#train-mlps)\n",
    "        1. [Data Preprocessing](#mlp-data)\n",
    "        2. [Building & Training](#fit-mlps)\n",
    "    3. [Training Transformers](#train-transformers)\n",
    "        1. [Data Preprocessing](#transformer-data)\n",
    "        2. [Building & Training](#fit-transformers)\n",
    "    4. [Finetuning (Optional)](#finetune)\n",
    "        1. [Finetuning an MLP](#finetune-mlp)\n",
    "        2. [Loading and Retraining](#load-tuner)\n",
    "4. [Testing Prediction Models](#test-models)\n",
    "    1. [Benchmark Networks](#benchmark-test)\n",
    "    2. [Plots](#plots)\n",
    "        1. [Box Plots](#box-plots)\n",
    "        2. [Bar Plots](#bar-plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General Usage <a class=\"anchor\" id=\"general-usage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Basic Usage <a class=\"anchor\" id=\"basic-usage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:47:37.676895: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-01 18:47:37.713414: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-01 18:47:37.713448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-01 18:47:37.714366: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-01 18:47:37.739984: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-01 18:47:39.861717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-09-01 18:47:41.447158: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Activation\n",
    "import keras\n",
    "\n",
    "# Example of a keras Model to predict\n",
    "input_size = 16\n",
    "inputs = Input(shape=(input_size,))\n",
    "x = Dense(32, activation=\"relu\")(inputs)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "outputs = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model_to_predict = keras.Model(inputs=inputs, outputs=outputs, name=\"Jet Classifier\")\n",
    "model_to_predict.build((None, input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import MultiModelEstimator\n",
    "\n",
    "# Load default estimator\n",
    "estimator = MultiModelEstimator()\n",
    "estimator.load_default_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mMultiModelEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      _summary_\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/relu4ml_hls4mlofficial/lib/python3.11/site-packages/rule4ml/models/estimators.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MultiModelEstimator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>BRAM (%)</th>\n",
       "      <th>DSP (%)</th>\n",
       "      <th>FF (%)</th>\n",
       "      <th>LUT (%)</th>\n",
       "      <th>CYCLES</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th>Board</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Reuse Factor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"126\" valign=\"top\">Jet Classifier</th>\n",
       "      <th rowspan=\"42\" valign=\"top\">pynq-z2</th>\n",
       "      <th rowspan=\"21\" valign=\"top\">Latency</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;2, 1&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>2.77</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.63</td>\n",
       "      <td>30.02</td>\n",
       "      <td>54.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.62</td>\n",
       "      <td>29.91</td>\n",
       "      <td>55.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.70</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.58</td>\n",
       "      <td>29.80</td>\n",
       "      <td>55.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.97</td>\n",
       "      <td>0.67</td>\n",
       "      <td>2.49</td>\n",
       "      <td>29.79</td>\n",
       "      <td>68.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.97</td>\n",
       "      <td>0.63</td>\n",
       "      <td>2.50</td>\n",
       "      <td>30.24</td>\n",
       "      <td>75.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.26</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.43</td>\n",
       "      <td>30.90</td>\n",
       "      <td>76.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.47</td>\n",
       "      <td>2.19</td>\n",
       "      <td>32.89</td>\n",
       "      <td>112.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;8, 3&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>2.63</td>\n",
       "      <td>1.58</td>\n",
       "      <td>13.91</td>\n",
       "      <td>115.89</td>\n",
       "      <td>53.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.63</td>\n",
       "      <td>1.50</td>\n",
       "      <td>13.63</td>\n",
       "      <td>111.75</td>\n",
       "      <td>54.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.59</td>\n",
       "      <td>1.25</td>\n",
       "      <td>13.07</td>\n",
       "      <td>108.52</td>\n",
       "      <td>56.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.76</td>\n",
       "      <td>1.41</td>\n",
       "      <td>12.22</td>\n",
       "      <td>108.01</td>\n",
       "      <td>53.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.42</td>\n",
       "      <td>1.96</td>\n",
       "      <td>11.98</td>\n",
       "      <td>104.58</td>\n",
       "      <td>64.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.99</td>\n",
       "      <td>1.93</td>\n",
       "      <td>12.74</td>\n",
       "      <td>94.71</td>\n",
       "      <td>83.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.56</td>\n",
       "      <td>1.70</td>\n",
       "      <td>14.74</td>\n",
       "      <td>92.78</td>\n",
       "      <td>104.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;16, 6&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>1.78</td>\n",
       "      <td>199.86</td>\n",
       "      <td>45.96</td>\n",
       "      <td>184.86</td>\n",
       "      <td>66.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.30</td>\n",
       "      <td>198.30</td>\n",
       "      <td>45.71</td>\n",
       "      <td>190.51</td>\n",
       "      <td>68.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.38</td>\n",
       "      <td>198.50</td>\n",
       "      <td>45.95</td>\n",
       "      <td>195.05</td>\n",
       "      <td>73.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.48</td>\n",
       "      <td>175.18</td>\n",
       "      <td>46.42</td>\n",
       "      <td>188.65</td>\n",
       "      <td>95.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.90</td>\n",
       "      <td>83.85</td>\n",
       "      <td>48.13</td>\n",
       "      <td>184.96</td>\n",
       "      <td>101.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.43</td>\n",
       "      <td>51.04</td>\n",
       "      <td>51.83</td>\n",
       "      <td>193.38</td>\n",
       "      <td>141.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.75</td>\n",
       "      <td>30.32</td>\n",
       "      <td>55.36</td>\n",
       "      <td>193.26</td>\n",
       "      <td>229.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"21\" valign=\"top\">Resource</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;2, 1&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>5.67</td>\n",
       "      <td>0.90</td>\n",
       "      <td>4.57</td>\n",
       "      <td>76.14</td>\n",
       "      <td>75.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.49</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4.56</td>\n",
       "      <td>149.70</td>\n",
       "      <td>78.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.99</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4.60</td>\n",
       "      <td>174.11</td>\n",
       "      <td>89.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.20</td>\n",
       "      <td>0.69</td>\n",
       "      <td>4.85</td>\n",
       "      <td>139.49</td>\n",
       "      <td>111.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.84</td>\n",
       "      <td>0.66</td>\n",
       "      <td>5.28</td>\n",
       "      <td>70.91</td>\n",
       "      <td>132.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.20</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4.46</td>\n",
       "      <td>39.63</td>\n",
       "      <td>208.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>3.94</td>\n",
       "      <td>45.27</td>\n",
       "      <td>304.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;8, 3&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>10.05</td>\n",
       "      <td>1.60</td>\n",
       "      <td>23.77</td>\n",
       "      <td>141.49</td>\n",
       "      <td>68.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55.80</td>\n",
       "      <td>1.52</td>\n",
       "      <td>22.87</td>\n",
       "      <td>183.85</td>\n",
       "      <td>73.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.67</td>\n",
       "      <td>1.27</td>\n",
       "      <td>20.98</td>\n",
       "      <td>198.83</td>\n",
       "      <td>77.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20.88</td>\n",
       "      <td>1.43</td>\n",
       "      <td>18.04</td>\n",
       "      <td>165.30</td>\n",
       "      <td>97.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>15.51</td>\n",
       "      <td>115.64</td>\n",
       "      <td>159.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.06</td>\n",
       "      <td>1.98</td>\n",
       "      <td>11.13</td>\n",
       "      <td>72.22</td>\n",
       "      <td>187.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3.01</td>\n",
       "      <td>1.75</td>\n",
       "      <td>9.30</td>\n",
       "      <td>67.29</td>\n",
       "      <td>333.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;16, 6&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>11.69</td>\n",
       "      <td>199.97</td>\n",
       "      <td>59.83</td>\n",
       "      <td>187.63</td>\n",
       "      <td>75.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119.10</td>\n",
       "      <td>198.44</td>\n",
       "      <td>57.58</td>\n",
       "      <td>195.18</td>\n",
       "      <td>75.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.59</td>\n",
       "      <td>198.63</td>\n",
       "      <td>52.50</td>\n",
       "      <td>197.65</td>\n",
       "      <td>83.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>47.71</td>\n",
       "      <td>175.83</td>\n",
       "      <td>43.00</td>\n",
       "      <td>196.30</td>\n",
       "      <td>122.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27.98</td>\n",
       "      <td>84.48</td>\n",
       "      <td>29.85</td>\n",
       "      <td>145.65</td>\n",
       "      <td>195.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9.69</td>\n",
       "      <td>51.36</td>\n",
       "      <td>20.54</td>\n",
       "      <td>116.51</td>\n",
       "      <td>239.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.45</td>\n",
       "      <td>30.57</td>\n",
       "      <td>15.79</td>\n",
       "      <td>117.74</td>\n",
       "      <td>361.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"42\" valign=\"top\">zcu102</th>\n",
       "      <th rowspan=\"21\" valign=\"top\">Latency</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;2, 1&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.57</td>\n",
       "      <td>36.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>7.47</td>\n",
       "      <td>37.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>7.50</td>\n",
       "      <td>41.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7.17</td>\n",
       "      <td>54.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.87</td>\n",
       "      <td>55.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.53</td>\n",
       "      <td>51.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.98</td>\n",
       "      <td>106.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;8, 3&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>26.85</td>\n",
       "      <td>41.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>26.28</td>\n",
       "      <td>42.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>26.49</td>\n",
       "      <td>44.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>26.13</td>\n",
       "      <td>46.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>25.94</td>\n",
       "      <td>45.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>25.22</td>\n",
       "      <td>56.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>23.51</td>\n",
       "      <td>74.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;16, 6&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>0.65</td>\n",
       "      <td>109.08</td>\n",
       "      <td>1.75</td>\n",
       "      <td>47.02</td>\n",
       "      <td>48.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.62</td>\n",
       "      <td>75.48</td>\n",
       "      <td>1.94</td>\n",
       "      <td>48.70</td>\n",
       "      <td>54.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.58</td>\n",
       "      <td>35.35</td>\n",
       "      <td>2.35</td>\n",
       "      <td>50.72</td>\n",
       "      <td>59.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.50</td>\n",
       "      <td>17.65</td>\n",
       "      <td>3.13</td>\n",
       "      <td>48.29</td>\n",
       "      <td>76.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.63</td>\n",
       "      <td>7.18</td>\n",
       "      <td>4.57</td>\n",
       "      <td>48.69</td>\n",
       "      <td>109.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.65</td>\n",
       "      <td>4.68</td>\n",
       "      <td>5.85</td>\n",
       "      <td>49.10</td>\n",
       "      <td>115.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.48</td>\n",
       "      <td>3.02</td>\n",
       "      <td>6.33</td>\n",
       "      <td>40.80</td>\n",
       "      <td>200.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"21\" valign=\"top\">Resource</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;2, 1&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>1.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>8.82</td>\n",
       "      <td>61.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>30.46</td>\n",
       "      <td>64.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>31.68</td>\n",
       "      <td>74.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>29.44</td>\n",
       "      <td>92.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>18.45</td>\n",
       "      <td>91.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>12.30</td>\n",
       "      <td>182.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.21</td>\n",
       "      <td>288.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;8, 3&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>26.10</td>\n",
       "      <td>55.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>54.43</td>\n",
       "      <td>59.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.02</td>\n",
       "      <td>72.25</td>\n",
       "      <td>62.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.14</td>\n",
       "      <td>48.05</td>\n",
       "      <td>78.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>28.90</td>\n",
       "      <td>141.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.27</td>\n",
       "      <td>14.83</td>\n",
       "      <td>179.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.28</td>\n",
       "      <td>14.77</td>\n",
       "      <td>296.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;16, 6&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>2.25</td>\n",
       "      <td>109.62</td>\n",
       "      <td>1.69</td>\n",
       "      <td>52.05</td>\n",
       "      <td>61.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.38</td>\n",
       "      <td>75.81</td>\n",
       "      <td>1.78</td>\n",
       "      <td>73.82</td>\n",
       "      <td>63.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.17</td>\n",
       "      <td>35.48</td>\n",
       "      <td>1.95</td>\n",
       "      <td>89.13</td>\n",
       "      <td>68.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.52</td>\n",
       "      <td>17.76</td>\n",
       "      <td>2.12</td>\n",
       "      <td>58.20</td>\n",
       "      <td>98.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.19</td>\n",
       "      <td>7.22</td>\n",
       "      <td>2.27</td>\n",
       "      <td>31.63</td>\n",
       "      <td>153.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.55</td>\n",
       "      <td>4.71</td>\n",
       "      <td>2.40</td>\n",
       "      <td>20.12</td>\n",
       "      <td>215.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.91</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.23</td>\n",
       "      <td>21.29</td>\n",
       "      <td>347.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"42\" valign=\"top\">alveo-u200</th>\n",
       "      <th rowspan=\"21\" valign=\"top\">Latency</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;2, 1&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>37.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>35.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>37.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.07</td>\n",
       "      <td>54.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>51.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>109.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;8, 3&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4.02</td>\n",
       "      <td>41.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>4.05</td>\n",
       "      <td>43.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4.30</td>\n",
       "      <td>44.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4.34</td>\n",
       "      <td>45.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4.57</td>\n",
       "      <td>42.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.93</td>\n",
       "      <td>53.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>4.60</td>\n",
       "      <td>68.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;16, 6&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>0.24</td>\n",
       "      <td>39.61</td>\n",
       "      <td>0.42</td>\n",
       "      <td>8.85</td>\n",
       "      <td>47.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21</td>\n",
       "      <td>25.37</td>\n",
       "      <td>0.48</td>\n",
       "      <td>9.35</td>\n",
       "      <td>53.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.16</td>\n",
       "      <td>12.97</td>\n",
       "      <td>0.62</td>\n",
       "      <td>9.48</td>\n",
       "      <td>58.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.08</td>\n",
       "      <td>6.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>9.52</td>\n",
       "      <td>76.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.19</td>\n",
       "      <td>2.74</td>\n",
       "      <td>1.27</td>\n",
       "      <td>9.71</td>\n",
       "      <td>111.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.48</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.47</td>\n",
       "      <td>9.03</td>\n",
       "      <td>114.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.45</td>\n",
       "      <td>8.64</td>\n",
       "      <td>195.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"21\" valign=\"top\">Resource</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;2, 1&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>61.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>6.10</td>\n",
       "      <td>64.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>6.91</td>\n",
       "      <td>72.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.58</td>\n",
       "      <td>89.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>91.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.66</td>\n",
       "      <td>176.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>283.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;8, 3&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>6.93</td>\n",
       "      <td>54.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>19.41</td>\n",
       "      <td>59.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>21.24</td>\n",
       "      <td>60.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>12.64</td>\n",
       "      <td>76.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>7.39</td>\n",
       "      <td>137.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2.02</td>\n",
       "      <td>164.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.56</td>\n",
       "      <td>291.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ap_fixed&lt;16, 6&gt;</th>\n",
       "      <th>1</th>\n",
       "      <td>2.06</td>\n",
       "      <td>39.82</td>\n",
       "      <td>0.32</td>\n",
       "      <td>12.43</td>\n",
       "      <td>59.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.51</td>\n",
       "      <td>25.47</td>\n",
       "      <td>0.33</td>\n",
       "      <td>17.46</td>\n",
       "      <td>61.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.39</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>16.43</td>\n",
       "      <td>67.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.24</td>\n",
       "      <td>6.59</td>\n",
       "      <td>0.43</td>\n",
       "      <td>10.71</td>\n",
       "      <td>96.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.46</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.54</td>\n",
       "      <td>6.80</td>\n",
       "      <td>148.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.73</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.49</td>\n",
       "      <td>3.55</td>\n",
       "      <td>212.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.67</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3.60</td>\n",
       "      <td>341.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 BRAM (%)  \\\n",
       "Model          Board      Strategy Precision       Reuse Factor             \n",
       "Jet Classifier pynq-z2    Latency  ap_fixed<2, 1>  1                 2.77   \n",
       "                                                   2                 2.75   \n",
       "                                                   4                 2.70   \n",
       "                                                   8                 2.97   \n",
       "                                                   16                2.97   \n",
       "                                                   32                2.26   \n",
       "                                                   64                0.83   \n",
       "                                   ap_fixed<8, 3>  1                 2.63   \n",
       "                                                   2                 2.63   \n",
       "                                                   4                 2.59   \n",
       "                                                   8                 2.76   \n",
       "                                                   16                3.42   \n",
       "                                                   32                2.99   \n",
       "                                                   64                0.56   \n",
       "                                   ap_fixed<16, 6> 1                 1.78   \n",
       "                                                   2                 2.30   \n",
       "                                                   4                 2.38   \n",
       "                                                   8                 1.48   \n",
       "                                                   16                2.90   \n",
       "                                                   32                4.43   \n",
       "                                                   64                0.75   \n",
       "                          Resource ap_fixed<2, 1>  1                 5.67   \n",
       "                                                   2                34.49   \n",
       "                                                   4                13.99   \n",
       "                                                   8                10.20   \n",
       "                                                   16                7.84   \n",
       "                                                   32                4.20   \n",
       "                                                   64                3.44   \n",
       "                                   ap_fixed<8, 3>  1                10.05   \n",
       "                                                   2                55.80   \n",
       "                                                   4                55.67   \n",
       "                                                   8                20.88   \n",
       "                                                   16               11.99   \n",
       "                                                   32                5.06   \n",
       "                                                   64                3.01   \n",
       "                                   ap_fixed<16, 6> 1                11.69   \n",
       "                                                   2               119.10   \n",
       "                                                   4                89.59   \n",
       "                                                   8                47.71   \n",
       "                                                   16               27.98   \n",
       "                                                   32                9.69   \n",
       "                                                   64                5.45   \n",
       "               zcu102     Latency  ap_fixed<2, 1>  1                 0.71   \n",
       "                                                   2                 0.71   \n",
       "                                                   4                 0.75   \n",
       "                                                   8                 0.84   \n",
       "                                                   16                0.81   \n",
       "                                                   32                0.38   \n",
       "                                                   64                0.54   \n",
       "                                   ap_fixed<8, 3>  1                 0.57   \n",
       "                                                   2                 0.54   \n",
       "                                                   4                 0.48   \n",
       "                                                   8                 0.47   \n",
       "                                                   16                0.72   \n",
       "                                                   32                0.62   \n",
       "                                                   64                0.55   \n",
       "                                   ap_fixed<16, 6> 1                 0.65   \n",
       "                                                   2                 0.62   \n",
       "                                                   4                 0.58   \n",
       "                                                   8                 0.50   \n",
       "                                                   16                0.63   \n",
       "                                                   32                0.65   \n",
       "                                                   64                0.48   \n",
       "                          Resource ap_fixed<2, 1>  1                 1.60   \n",
       "                                                   2                 3.81   \n",
       "                                                   4                 2.68   \n",
       "                                                   8                 1.11   \n",
       "                                                   16                0.85   \n",
       "                                                   32                0.74   \n",
       "                                                   64                0.73   \n",
       "                                   ap_fixed<8, 3>  1                 2.04   \n",
       "                                                   2                12.17   \n",
       "                                                   4                 6.98   \n",
       "                                                   8                 5.40   \n",
       "                                                   16                1.46   \n",
       "                                                   32                0.94   \n",
       "                                                   64                0.77   \n",
       "                                   ap_fixed<16, 6> 1                 2.25   \n",
       "                                                   2                32.38   \n",
       "                                                   4                21.17   \n",
       "                                                   8                11.52   \n",
       "                                                   16                5.19   \n",
       "                                                   32                1.55   \n",
       "                                                   64                0.91   \n",
       "               alveo-u200 Latency  ap_fixed<2, 1>  1                 0.81   \n",
       "                                                   2                 0.83   \n",
       "                                                   4                 0.84   \n",
       "                                                   8                 0.83   \n",
       "                                                   16                0.85   \n",
       "                                                   32                0.41   \n",
       "                                                   64                0.30   \n",
       "                                   ap_fixed<8, 3>  1                 0.42   \n",
       "                                                   2                 0.39   \n",
       "                                                   4                 0.33   \n",
       "                                                   8                 0.42   \n",
       "                                                   16                0.56   \n",
       "                                                   32                0.36   \n",
       "                                                   64                0.15   \n",
       "                                   ap_fixed<16, 6> 1                 0.24   \n",
       "                                                   2                 0.21   \n",
       "                                                   4                 0.16   \n",
       "                                                   8                 0.08   \n",
       "                                                   16                0.19   \n",
       "                                                   32                0.48   \n",
       "                                                   64                0.10   \n",
       "                          Resource ap_fixed<2, 1>  1                 0.78   \n",
       "                                                   2                 1.32   \n",
       "                                                   4                 1.11   \n",
       "                                                   8                 0.77   \n",
       "                                                   16                0.49   \n",
       "                                                   32                0.41   \n",
       "                                                   64                0.45   \n",
       "                                   ap_fixed<8, 3>  1                 0.86   \n",
       "                                                   2                 3.56   \n",
       "                                                   4                 2.46   \n",
       "                                                   8                 1.39   \n",
       "                                                   16                0.51   \n",
       "                                                   32                0.43   \n",
       "                                                   64                0.49   \n",
       "                                   ap_fixed<16, 6> 1                 2.06   \n",
       "                                                   2                 5.51   \n",
       "                                                   4                 4.39   \n",
       "                                                   8                 2.24   \n",
       "                                                   16                1.46   \n",
       "                                                   32                0.73   \n",
       "                                                   64                0.67   \n",
       "\n",
       "                                                                 DSP (%)  \\\n",
       "Model          Board      Strategy Precision       Reuse Factor            \n",
       "Jet Classifier pynq-z2    Latency  ap_fixed<2, 1>  1                0.89   \n",
       "                                                   2                0.86   \n",
       "                                                   4                0.79   \n",
       "                                                   8                0.67   \n",
       "                                                   16               0.63   \n",
       "                                                   32               0.74   \n",
       "                                                   64               0.47   \n",
       "                                   ap_fixed<8, 3>  1                1.58   \n",
       "                                                   2                1.50   \n",
       "                                                   4                1.25   \n",
       "                                                   8                1.41   \n",
       "                                                   16               1.96   \n",
       "                                                   32               1.93   \n",
       "                                                   64               1.70   \n",
       "                                   ap_fixed<16, 6> 1              199.86   \n",
       "                                                   2              198.30   \n",
       "                                                   4              198.50   \n",
       "                                                   8              175.18   \n",
       "                                                   16              83.85   \n",
       "                                                   32              51.04   \n",
       "                                                   64              30.32   \n",
       "                          Resource ap_fixed<2, 1>  1                0.90   \n",
       "                                                   2                0.89   \n",
       "                                                   4                0.82   \n",
       "                                                   8                0.69   \n",
       "                                                   16               0.66   \n",
       "                                                   32               0.77   \n",
       "                                                   64               0.48   \n",
       "                                   ap_fixed<8, 3>  1                1.60   \n",
       "                                                   2                1.52   \n",
       "                                                   4                1.27   \n",
       "                                                   8                1.43   \n",
       "                                                   16               2.00   \n",
       "                                                   32               1.98   \n",
       "                                                   64               1.75   \n",
       "                                   ap_fixed<16, 6> 1              199.97   \n",
       "                                                   2              198.44   \n",
       "                                                   4              198.63   \n",
       "                                                   8              175.83   \n",
       "                                                   16              84.48   \n",
       "                                                   32              51.36   \n",
       "                                                   64              30.57   \n",
       "               zcu102     Latency  ap_fixed<2, 1>  1                0.00   \n",
       "                                                   2                0.00   \n",
       "                                                   4                0.00   \n",
       "                                                   8                0.00   \n",
       "                                                   16               0.00   \n",
       "                                                   32               0.00   \n",
       "                                                   64               0.00   \n",
       "                                   ap_fixed<8, 3>  1                0.00   \n",
       "                                                   2                0.00   \n",
       "                                                   4                0.00   \n",
       "                                                   8                0.00   \n",
       "                                                   16               0.00   \n",
       "                                                   32               0.00   \n",
       "                                                   64               0.00   \n",
       "                                   ap_fixed<16, 6> 1              109.08   \n",
       "                                                   2               75.48   \n",
       "                                                   4               35.35   \n",
       "                                                   8               17.65   \n",
       "                                                   16               7.18   \n",
       "                                                   32               4.68   \n",
       "                                                   64               3.02   \n",
       "                          Resource ap_fixed<2, 1>  1                0.00   \n",
       "                                                   2                0.00   \n",
       "                                                   4                0.00   \n",
       "                                                   8                0.00   \n",
       "                                                   16               0.00   \n",
       "                                                   32               0.00   \n",
       "                                                   64               0.00   \n",
       "                                   ap_fixed<8, 3>  1                0.00   \n",
       "                                                   2                0.00   \n",
       "                                                   4                0.00   \n",
       "                                                   8                0.00   \n",
       "                                                   16               0.00   \n",
       "                                                   32               0.00   \n",
       "                                                   64               0.00   \n",
       "                                   ap_fixed<16, 6> 1              109.62   \n",
       "                                                   2               75.81   \n",
       "                                                   4               35.48   \n",
       "                                                   8               17.76   \n",
       "                                                   16               7.22   \n",
       "                                                   32               4.71   \n",
       "                                                   64               3.03   \n",
       "               alveo-u200 Latency  ap_fixed<2, 1>  1                0.00   \n",
       "                                                   2                0.00   \n",
       "                                                   4                0.00   \n",
       "                                                   8                0.00   \n",
       "                                                   16               0.00   \n",
       "                                                   32               0.00   \n",
       "                                                   64               0.00   \n",
       "                                   ap_fixed<8, 3>  1                0.00   \n",
       "                                                   2                0.00   \n",
       "                                                   4                0.00   \n",
       "                                                   8                0.00   \n",
       "                                                   16               0.00   \n",
       "                                                   32               0.00   \n",
       "                                                   64               0.00   \n",
       "                                   ap_fixed<16, 6> 1               39.61   \n",
       "                                                   2               25.37   \n",
       "                                                   4               12.97   \n",
       "                                                   8                6.57   \n",
       "                                                   16               2.74   \n",
       "                                                   32               1.81   \n",
       "                                                   64               1.25   \n",
       "                          Resource ap_fixed<2, 1>  1                0.00   \n",
       "                                                   2                0.00   \n",
       "                                                   4                0.00   \n",
       "                                                   8                0.00   \n",
       "                                                   16               0.00   \n",
       "                                                   32               0.00   \n",
       "                                                   64               0.00   \n",
       "                                   ap_fixed<8, 3>  1                0.00   \n",
       "                                                   2                0.00   \n",
       "                                                   4                0.00   \n",
       "                                                   8                0.00   \n",
       "                                                   16               0.00   \n",
       "                                                   32               0.00   \n",
       "                                                   64               0.00   \n",
       "                                   ap_fixed<16, 6> 1               39.82   \n",
       "                                                   2               25.47   \n",
       "                                                   4               13.02   \n",
       "                                                   8                6.59   \n",
       "                                                   16               2.76   \n",
       "                                                   32               1.81   \n",
       "                                                   64               1.25   \n",
       "\n",
       "                                                                 FF (%)  \\\n",
       "Model          Board      Strategy Precision       Reuse Factor           \n",
       "Jet Classifier pynq-z2    Latency  ap_fixed<2, 1>  1               2.63   \n",
       "                                                   2               2.62   \n",
       "                                                   4               2.58   \n",
       "                                                   8               2.49   \n",
       "                                                   16              2.50   \n",
       "                                                   32              2.43   \n",
       "                                                   64              2.19   \n",
       "                                   ap_fixed<8, 3>  1              13.91   \n",
       "                                                   2              13.63   \n",
       "                                                   4              13.07   \n",
       "                                                   8              12.22   \n",
       "                                                   16             11.98   \n",
       "                                                   32             12.74   \n",
       "                                                   64             14.74   \n",
       "                                   ap_fixed<16, 6> 1              45.96   \n",
       "                                                   2              45.71   \n",
       "                                                   4              45.95   \n",
       "                                                   8              46.42   \n",
       "                                                   16             48.13   \n",
       "                                                   32             51.83   \n",
       "                                                   64             55.36   \n",
       "                          Resource ap_fixed<2, 1>  1               4.57   \n",
       "                                                   2               4.56   \n",
       "                                                   4               4.60   \n",
       "                                                   8               4.85   \n",
       "                                                   16              5.28   \n",
       "                                                   32              4.46   \n",
       "                                                   64              3.94   \n",
       "                                   ap_fixed<8, 3>  1              23.77   \n",
       "                                                   2              22.87   \n",
       "                                                   4              20.98   \n",
       "                                                   8              18.04   \n",
       "                                                   16             15.51   \n",
       "                                                   32             11.13   \n",
       "                                                   64              9.30   \n",
       "                                   ap_fixed<16, 6> 1              59.83   \n",
       "                                                   2              57.58   \n",
       "                                                   4              52.50   \n",
       "                                                   8              43.00   \n",
       "                                                   16             29.85   \n",
       "                                                   32             20.54   \n",
       "                                                   64             15.79   \n",
       "               zcu102     Latency  ap_fixed<2, 1>  1               0.16   \n",
       "                                                   2               0.17   \n",
       "                                                   4               0.19   \n",
       "                                                   8               0.21   \n",
       "                                                   16              0.12   \n",
       "                                                   32              0.00   \n",
       "                                                   64              0.01   \n",
       "                                   ap_fixed<8, 3>  1               0.17   \n",
       "                                                   2               0.20   \n",
       "                                                   4               0.23   \n",
       "                                                   8               0.30   \n",
       "                                                   16              0.47   \n",
       "                                                   32              0.68   \n",
       "                                                   64              0.88   \n",
       "                                   ap_fixed<16, 6> 1               1.75   \n",
       "                                                   2               1.94   \n",
       "                                                   4               2.35   \n",
       "                                                   8               3.13   \n",
       "                                                   16              4.57   \n",
       "                                                   32              5.85   \n",
       "                                                   64              6.33   \n",
       "                          Resource ap_fixed<2, 1>  1               0.53   \n",
       "                                                   2               0.52   \n",
       "                                                   4               0.49   \n",
       "                                                   8               0.40   \n",
       "                                                   16              0.33   \n",
       "                                                   32              0.33   \n",
       "                                                   64              0.40   \n",
       "                                   ap_fixed<8, 3>  1               0.93   \n",
       "                                                   2               0.96   \n",
       "                                                   4               1.02   \n",
       "                                                   8               1.14   \n",
       "                                                   16              1.22   \n",
       "                                                   32              1.27   \n",
       "                                                   64              1.28   \n",
       "                                   ap_fixed<16, 6> 1               1.69   \n",
       "                                                   2               1.78   \n",
       "                                                   4               1.95   \n",
       "                                                   8               2.12   \n",
       "                                                   16              2.27   \n",
       "                                                   32              2.40   \n",
       "                                                   64              2.23   \n",
       "               alveo-u200 Latency  ap_fixed<2, 1>  1               0.00   \n",
       "                                                   2               0.00   \n",
       "                                                   4               0.00   \n",
       "                                                   8               0.00   \n",
       "                                                   16              0.00   \n",
       "                                                   32              0.00   \n",
       "                                                   64              0.00   \n",
       "                                   ap_fixed<8, 3>  1               0.16   \n",
       "                                                   2               0.17   \n",
       "                                                   4               0.18   \n",
       "                                                   8               0.19   \n",
       "                                                   16              0.19   \n",
       "                                                   32              0.21   \n",
       "                                                   64              0.17   \n",
       "                                   ap_fixed<16, 6> 1               0.42   \n",
       "                                                   2               0.48   \n",
       "                                                   4               0.62   \n",
       "                                                   8               0.86   \n",
       "                                                   16              1.27   \n",
       "                                                   32              1.47   \n",
       "                                                   64              1.45   \n",
       "                          Resource ap_fixed<2, 1>  1               0.22   \n",
       "                                                   2               0.22   \n",
       "                                                   4               0.23   \n",
       "                                                   8               0.22   \n",
       "                                                   16              0.20   \n",
       "                                                   32              0.17   \n",
       "                                                   64              0.16   \n",
       "                                   ap_fixed<8, 3>  1               0.26   \n",
       "                                                   2               0.27   \n",
       "                                                   4               0.29   \n",
       "                                                   8               0.31   \n",
       "                                                   16              0.37   \n",
       "                                                   32              0.34   \n",
       "                                                   64              0.19   \n",
       "                                   ap_fixed<16, 6> 1               0.32   \n",
       "                                                   2               0.33   \n",
       "                                                   4               0.37   \n",
       "                                                   8               0.43   \n",
       "                                                   16              0.54   \n",
       "                                                   32              0.49   \n",
       "                                                   64              0.64   \n",
       "\n",
       "                                                                 LUT (%)  \\\n",
       "Model          Board      Strategy Precision       Reuse Factor            \n",
       "Jet Classifier pynq-z2    Latency  ap_fixed<2, 1>  1               30.02   \n",
       "                                                   2               29.91   \n",
       "                                                   4               29.80   \n",
       "                                                   8               29.79   \n",
       "                                                   16              30.24   \n",
       "                                                   32              30.90   \n",
       "                                                   64              32.89   \n",
       "                                   ap_fixed<8, 3>  1              115.89   \n",
       "                                                   2              111.75   \n",
       "                                                   4              108.52   \n",
       "                                                   8              108.01   \n",
       "                                                   16             104.58   \n",
       "                                                   32              94.71   \n",
       "                                                   64              92.78   \n",
       "                                   ap_fixed<16, 6> 1              184.86   \n",
       "                                                   2              190.51   \n",
       "                                                   4              195.05   \n",
       "                                                   8              188.65   \n",
       "                                                   16             184.96   \n",
       "                                                   32             193.38   \n",
       "                                                   64             193.26   \n",
       "                          Resource ap_fixed<2, 1>  1               76.14   \n",
       "                                                   2              149.70   \n",
       "                                                   4              174.11   \n",
       "                                                   8              139.49   \n",
       "                                                   16              70.91   \n",
       "                                                   32              39.63   \n",
       "                                                   64              45.27   \n",
       "                                   ap_fixed<8, 3>  1              141.49   \n",
       "                                                   2              183.85   \n",
       "                                                   4              198.83   \n",
       "                                                   8              165.30   \n",
       "                                                   16             115.64   \n",
       "                                                   32              72.22   \n",
       "                                                   64              67.29   \n",
       "                                   ap_fixed<16, 6> 1              187.63   \n",
       "                                                   2              195.18   \n",
       "                                                   4              197.65   \n",
       "                                                   8              196.30   \n",
       "                                                   16             145.65   \n",
       "                                                   32             116.51   \n",
       "                                                   64             117.74   \n",
       "               zcu102     Latency  ap_fixed<2, 1>  1                7.57   \n",
       "                                                   2                7.47   \n",
       "                                                   4                7.50   \n",
       "                                                   8                7.17   \n",
       "                                                   16               5.87   \n",
       "                                                   32               4.53   \n",
       "                                                   64               6.98   \n",
       "                                   ap_fixed<8, 3>  1               26.85   \n",
       "                                                   2               26.28   \n",
       "                                                   4               26.49   \n",
       "                                                   8               26.13   \n",
       "                                                   16              25.94   \n",
       "                                                   32              25.22   \n",
       "                                                   64              23.51   \n",
       "                                   ap_fixed<16, 6> 1               47.02   \n",
       "                                                   2               48.70   \n",
       "                                                   4               50.72   \n",
       "                                                   8               48.29   \n",
       "                                                   16              48.69   \n",
       "                                                   32              49.10   \n",
       "                                                   64              40.80   \n",
       "                          Resource ap_fixed<2, 1>  1                8.82   \n",
       "                                                   2               30.46   \n",
       "                                                   4               31.68   \n",
       "                                                   8               29.44   \n",
       "                                                   16              18.45   \n",
       "                                                   32              12.30   \n",
       "                                                   64               9.21   \n",
       "                                   ap_fixed<8, 3>  1               26.10   \n",
       "                                                   2               54.43   \n",
       "                                                   4               72.25   \n",
       "                                                   8               48.05   \n",
       "                                                   16              28.90   \n",
       "                                                   32              14.83   \n",
       "                                                   64              14.77   \n",
       "                                   ap_fixed<16, 6> 1               52.05   \n",
       "                                                   2               73.82   \n",
       "                                                   4               89.13   \n",
       "                                                   8               58.20   \n",
       "                                                   16              31.63   \n",
       "                                                   32              20.12   \n",
       "                                                   64              21.29   \n",
       "               alveo-u200 Latency  ap_fixed<2, 1>  1                0.75   \n",
       "                                                   2                0.53   \n",
       "                                                   4                0.67   \n",
       "                                                   8                0.68   \n",
       "                                                   16               1.07   \n",
       "                                                   32               0.00   \n",
       "                                                   64               0.00   \n",
       "                                   ap_fixed<8, 3>  1                4.02   \n",
       "                                                   2                4.05   \n",
       "                                                   4                4.30   \n",
       "                                                   8                4.34   \n",
       "                                                   16               4.57   \n",
       "                                                   32               3.93   \n",
       "                                                   64               4.60   \n",
       "                                   ap_fixed<16, 6> 1                8.85   \n",
       "                                                   2                9.35   \n",
       "                                                   4                9.48   \n",
       "                                                   8                9.52   \n",
       "                                                   16               9.71   \n",
       "                                                   32               9.03   \n",
       "                                                   64               8.64   \n",
       "                          Resource ap_fixed<2, 1>  1                0.00   \n",
       "                                                   2                6.10   \n",
       "                                                   4                6.91   \n",
       "                                                   8                3.58   \n",
       "                                                   16               0.00   \n",
       "                                                   32               0.66   \n",
       "                                                   64               0.00   \n",
       "                                   ap_fixed<8, 3>  1                6.93   \n",
       "                                                   2               19.41   \n",
       "                                                   4               21.24   \n",
       "                                                   8               12.64   \n",
       "                                                   16               7.39   \n",
       "                                                   32               2.02   \n",
       "                                                   64               2.56   \n",
       "                                   ap_fixed<16, 6> 1               12.43   \n",
       "                                                   2               17.46   \n",
       "                                                   4               16.43   \n",
       "                                                   8               10.71   \n",
       "                                                   16               6.80   \n",
       "                                                   32               3.55   \n",
       "                                                   64               3.60   \n",
       "\n",
       "                                                                 CYCLES  \n",
       "Model          Board      Strategy Precision       Reuse Factor          \n",
       "Jet Classifier pynq-z2    Latency  ap_fixed<2, 1>  1              54.68  \n",
       "                                                   2              55.84  \n",
       "                                                   4              55.78  \n",
       "                                                   8              68.84  \n",
       "                                                   16             75.38  \n",
       "                                                   32             76.19  \n",
       "                                                   64            112.04  \n",
       "                                   ap_fixed<8, 3>  1              53.96  \n",
       "                                                   2              54.70  \n",
       "                                                   4              56.16  \n",
       "                                                   8              53.07  \n",
       "                                                   16             64.71  \n",
       "                                                   32             83.06  \n",
       "                                                   64            104.88  \n",
       "                                   ap_fixed<16, 6> 1              66.59  \n",
       "                                                   2              68.14  \n",
       "                                                   4              73.15  \n",
       "                                                   8              95.70  \n",
       "                                                   16            101.44  \n",
       "                                                   32            141.07  \n",
       "                                                   64            229.37  \n",
       "                          Resource ap_fixed<2, 1>  1              75.22  \n",
       "                                                   2              78.89  \n",
       "                                                   4              89.72  \n",
       "                                                   8             111.75  \n",
       "                                                   16            132.12  \n",
       "                                                   32            208.69  \n",
       "                                                   64            304.05  \n",
       "                                   ap_fixed<8, 3>  1              68.06  \n",
       "                                                   2              73.07  \n",
       "                                                   4              77.51  \n",
       "                                                   8              97.81  \n",
       "                                                   16            159.26  \n",
       "                                                   32            187.69  \n",
       "                                                   64            333.98  \n",
       "                                   ap_fixed<16, 6> 1              75.57  \n",
       "                                                   2              75.91  \n",
       "                                                   4              83.27  \n",
       "                                                   8             122.93  \n",
       "                                                   16            195.46  \n",
       "                                                   32            239.00  \n",
       "                                                   64            361.06  \n",
       "               zcu102     Latency  ap_fixed<2, 1>  1              36.51  \n",
       "                                                   2              37.45  \n",
       "                                                   4              41.43  \n",
       "                                                   8              54.65  \n",
       "                                                   16             55.57  \n",
       "                                                   32             51.70  \n",
       "                                                   64            106.39  \n",
       "                                   ap_fixed<8, 3>  1              41.56  \n",
       "                                                   2              42.61  \n",
       "                                                   4              44.76  \n",
       "                                                   8              46.50  \n",
       "                                                   16             45.63  \n",
       "                                                   32             56.56  \n",
       "                                                   64             74.10  \n",
       "                                   ap_fixed<16, 6> 1              48.31  \n",
       "                                                   2              54.69  \n",
       "                                                   4              59.01  \n",
       "                                                   8              76.58  \n",
       "                                                   16            109.89  \n",
       "                                                   32            115.60  \n",
       "                                                   64            200.85  \n",
       "                          Resource ap_fixed<2, 1>  1              61.64  \n",
       "                                                   2              64.96  \n",
       "                                                   4              74.60  \n",
       "                                                   8              92.17  \n",
       "                                                   16             91.11  \n",
       "                                                   32            182.08  \n",
       "                                                   64            288.58  \n",
       "                                   ap_fixed<8, 3>  1              55.23  \n",
       "                                                   2              59.48  \n",
       "                                                   4              62.33  \n",
       "                                                   8              78.71  \n",
       "                                                   16            141.35  \n",
       "                                                   32            179.51  \n",
       "                                                   64            296.61  \n",
       "                                   ap_fixed<16, 6> 1              61.63  \n",
       "                                                   2              63.53  \n",
       "                                                   4              68.18  \n",
       "                                                   8              98.30  \n",
       "                                                   16            153.89  \n",
       "                                                   32            215.93  \n",
       "                                                   64            347.91  \n",
       "               alveo-u200 Latency  ap_fixed<2, 1>  1              37.91  \n",
       "                                                   2              35.80  \n",
       "                                                   4              37.74  \n",
       "                                                   8              53.28  \n",
       "                                                   16             54.17  \n",
       "                                                   32             51.97  \n",
       "                                                   64            109.17  \n",
       "                                   ap_fixed<8, 3>  1              41.68  \n",
       "                                                   2              43.37  \n",
       "                                                   4              44.61  \n",
       "                                                   8              45.71  \n",
       "                                                   16             42.68  \n",
       "                                                   32             53.96  \n",
       "                                                   64             68.71  \n",
       "                                   ap_fixed<16, 6> 1              47.30  \n",
       "                                                   2              53.96  \n",
       "                                                   4              58.21  \n",
       "                                                   8              76.48  \n",
       "                                                   16            111.29  \n",
       "                                                   32            114.66  \n",
       "                                                   64            195.29  \n",
       "                          Resource ap_fixed<2, 1>  1              61.10  \n",
       "                                                   2              64.01  \n",
       "                                                   4              72.46  \n",
       "                                                   8              89.42  \n",
       "                                                   16             91.14  \n",
       "                                                   32            176.23  \n",
       "                                                   64            283.55  \n",
       "                                   ap_fixed<8, 3>  1              54.84  \n",
       "                                                   2              59.33  \n",
       "                                                   4              60.40  \n",
       "                                                   8              76.54  \n",
       "                                                   16            137.63  \n",
       "                                                   32            164.63  \n",
       "                                                   64            291.64  \n",
       "                                   ap_fixed<16, 6> 1              59.63  \n",
       "                                                   2              61.36  \n",
       "                                                   4              67.35  \n",
       "                                                   8              96.51  \n",
       "                                                   16            148.54  \n",
       "                                                   32            212.15  \n",
       "                                                   64            341.34  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MultiModelEstimator predictions are formatted as a DataFrame\n",
    "prediction_df = estimator.predict(model_to_predict)\n",
    "\n",
    "# each row is unique in the groupby, mean() is only called to convert DataFrameGroupBy into a nicely organized DataFrame\n",
    "if not prediction_df.empty:\n",
    "    prediction_df = prediction_df.groupby(\n",
    "        [\"Model\", \"Board\", \"Strategy\", \"Precision\", \"Reuse Factor\"], observed=True\n",
    "    ).mean()\n",
    "\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_df.to_html(\"keras_example.html\")\n",
    "\n",
    "# prediction_df.to_latex(\"keras_example.tex\")\n",
    "# prediction_df.to_csv(\"keras_example.csv\")\n",
    "# prediction_df.to_json(\"keras_example.json\")\n",
    "# prediction_df.to_xml(\"keras_example.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Advanced Usage <a class=\"anchor\" id=\"advanced-usage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:55:43.715958: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-01 18:55:43.761319: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-01 18:55:43.761353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-01 18:55:43.762423: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-01 18:55:43.769610: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-01 18:55:45.695390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-09-01 18:55:48.082635: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "import torch\n",
    "\n",
    "models_to_predict = []\n",
    "\n",
    "\n",
    "# Example of a subclassed PyTorch model\n",
    "class MyTopQuarks(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyTopQuarks, self).__init__()\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(10, 32)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dense2 = torch.nn.Linear(32, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        outputs = self.sigmoid(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "models_to_predict.append(MyTopQuarks())\n",
    "\n",
    "# Example of a keras Sequential model\n",
    "input_size = 16\n",
    "model_to_predict = keras.Sequential(\n",
    "    layers=[\n",
    "        keras.layers.Input(shape=(input_size,)),\n",
    "        keras.layers.Dense(32, use_bias=True),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        keras.layers.Dense(32, use_bias=True),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        keras.layers.Dense(32, use_bias=True),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        keras.layers.Dense(5, use_bias=True),\n",
    "        keras.layers.Activation(\"softmax\"),\n",
    "    ],\n",
    "    name=\"Jet Classifier\",\n",
    ")\n",
    "model_to_predict.build((None, input_size))\n",
    "\n",
    "models_to_predict.append(model_to_predict)\n",
    "\n",
    "hls_configs = [\n",
    "    {\n",
    "        \"model\": {\n",
    "            \"precision\": \"ap_fixed<8, 3>\",\n",
    "            \"reuse_factor\": 32,\n",
    "            \"strategy\": strategy,\n",
    "        },\n",
    "        \"board\": board,\n",
    "    }\n",
    "    for board, strategy in itertools.product([\"pynq-z2\", \"zcu102\"], [\"Latency\", \"Resource\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.422424 ],\n",
       "       [51.593323 ],\n",
       "       [ 3.6865003],\n",
       "       [ 6.612974 ],\n",
       "       [94.71447  ],\n",
       "       [72.222435 ],\n",
       "       [25.216333 ],\n",
       "       [14.825675 ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "lut_model_wrapper = ModelWrapper()\n",
    "lut_model_wrapper.load(\"./models/best_LUT_MLP_config.json\", \"./models/best_LUT_MLP.weights.h5\")\n",
    "\n",
    "lut_model_wrapper.predict(models_to_predict, hls_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Board</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Reuse Factor</th>\n",
       "      <th>LUT (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MyTopQuarks</td>\n",
       "      <td>pynq-z2</td>\n",
       "      <td>Latency</td>\n",
       "      <td>ap_fixed&lt;8, 3&gt;</td>\n",
       "      <td>32</td>\n",
       "      <td>39.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MyTopQuarks</td>\n",
       "      <td>pynq-z2</td>\n",
       "      <td>Resource</td>\n",
       "      <td>ap_fixed&lt;8, 3&gt;</td>\n",
       "      <td>32</td>\n",
       "      <td>51.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MyTopQuarks</td>\n",
       "      <td>zcu102</td>\n",
       "      <td>Latency</td>\n",
       "      <td>ap_fixed&lt;8, 3&gt;</td>\n",
       "      <td>32</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MyTopQuarks</td>\n",
       "      <td>zcu102</td>\n",
       "      <td>Resource</td>\n",
       "      <td>ap_fixed&lt;8, 3&gt;</td>\n",
       "      <td>32</td>\n",
       "      <td>6.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jet Classifier</td>\n",
       "      <td>pynq-z2</td>\n",
       "      <td>Latency</td>\n",
       "      <td>ap_fixed&lt;8, 3&gt;</td>\n",
       "      <td>32</td>\n",
       "      <td>94.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jet Classifier</td>\n",
       "      <td>pynq-z2</td>\n",
       "      <td>Resource</td>\n",
       "      <td>ap_fixed&lt;8, 3&gt;</td>\n",
       "      <td>32</td>\n",
       "      <td>72.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jet Classifier</td>\n",
       "      <td>zcu102</td>\n",
       "      <td>Latency</td>\n",
       "      <td>ap_fixed&lt;8, 3&gt;</td>\n",
       "      <td>32</td>\n",
       "      <td>25.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jet Classifier</td>\n",
       "      <td>zcu102</td>\n",
       "      <td>Resource</td>\n",
       "      <td>ap_fixed&lt;8, 3&gt;</td>\n",
       "      <td>32</td>\n",
       "      <td>14.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model    Board  Strategy       Precision  Reuse Factor  LUT (%)\n",
       "0     MyTopQuarks  pynq-z2   Latency  ap_fixed<8, 3>            32    39.42\n",
       "1     MyTopQuarks  pynq-z2  Resource  ap_fixed<8, 3>            32    51.59\n",
       "2     MyTopQuarks   zcu102   Latency  ap_fixed<8, 3>            32     3.69\n",
       "3     MyTopQuarks   zcu102  Resource  ap_fixed<8, 3>            32     6.61\n",
       "4  Jet Classifier  pynq-z2   Latency  ap_fixed<8, 3>            32    94.71\n",
       "5  Jet Classifier  pynq-z2  Resource  ap_fixed<8, 3>            32    72.22\n",
       "6  Jet Classifier   zcu102   Latency  ap_fixed<8, 3>            32    25.22\n",
       "7  Jet Classifier   zcu102  Resource  ap_fixed<8, 3>            32    14.83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rule4ml.models.estimators import MultiModelEstimator\n",
    "\n",
    "estimator = MultiModelEstimator()\n",
    "estimator.add_model_wrapper(lut_model_wrapper)\n",
    "\n",
    "estimator.predict(models_to_predict, hls_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Generation <a class=\"anchor\" id=\"data-gen\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Specify Vivado path\n",
    "#os.environ[\"PATH\"] = \"/opt/Xilinx/Vivado/2019.1/bin:\" + os.environ[\"PATH\"]\n",
    "#os.environ[\"PATH\"]  = \"/home/share/Xilinx/Vivado/2023.2/bin\" + os.environ[\"PATH\"]\n",
    "os.environ[\"PATH\"]  = \"/home/share/Xilinx/Vitis_HLS/2023.2/bin/\" + os.environ[\"PATH\"]\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\", \"data_gen\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model Synthesis <a class=\"anchor\" id=\"model-synth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Keras Model <a class=\"anchor\" id=\"keras-synth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 20:41:03.416783: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-01 20:41:03.460998: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-01 20:41:03.461029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-01 20:41:03.462117: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-01 20:41:03.469062: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-01 20:41:04.208000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-09-01 20:41:06.606320: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/crchen/anaconda3/envs/relu4ml_hls4mlofficial/lib/python3.11/site-packages/hls4ml/backends/fpga/passes/fix_softmax_table_size.py:34: UserWarning: Softmax layer dense_3_softmax table size is too large for inputbitwidth 8. Setting table size to 256.To avoid this warning, please increase input bitwidth ordecrease table size.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 16]], output shape: [None, 16]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 16]], output shape: [None, 32]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 5]\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 16]], output shape: [None, 16]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 16]], output shape: [None, 32]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 5]\n",
      "Creating HLS model\n",
      "WARNING: Changing pipeline style to \"dataflow\".\n",
      "Writing HLS project\n",
      "Done\n",
      "\n",
      "****** Vitis HLS - High-Level Synthesis from C, C++ and OpenCL v2023.2 (64-bit)\n",
      "  **** SW Build 4023990 on Oct 11 2023\n",
      "  **** IP Build 4028589 on Sat Oct 14 00:45:43 MDT 2023\n",
      "  **** SharedData Build 4025554 on Tue Oct 10 17:18:54 MDT 2023\n",
      "    ** Copyright 1986-2022 Xilinx, Inc. All Rights Reserved.\n",
      "    ** Copyright 2022-2023 Advanced Micro Devices, Inc. All Rights Reserved.\n",
      "\n",
      "source /home/share/Xilinx/Vitis_HLS/2023.2/scripts/vitis_hls/hls.tcl -notrace\n",
      "INFO: [HLS 200-10] Running '/home/share/Xilinx/Vitis_HLS/2023.2/bin/unwrapped/lnx64.o/vitis_hls'\n",
      "INFO: [HLS 200-10] For user 'crchen' on host 'Kaohsiung' (Linux_x86_64 version 5.15.0-100-generic) on Sun Sep 01 20:41:11 CST 2024\n",
      "INFO: [HLS 200-10] On os Ubuntu 20.04.6 LTS\n",
      "INFO: [HLS 200-10] In directory '/home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj'\n",
      "INFO: [HLS 200-2053] The vitis_hls executable is being deprecated. Consider using vitis-run --mode hls --tcl\n",
      "Sourcing Tcl script 'build_prj.tcl'\n",
      "INFO: [HLS 200-1510] Running: open_project myproject_prj \n",
      "INFO: [HLS 200-10] Creating and opening project '/home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj'.\n",
      "INFO: [HLS 200-1510] Running: set_top myproject \n",
      "INFO: [HLS 200-1510] Running: add_files firmware/myproject.cpp -cflags -std=c++0x \n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-1510] Running: add_files -tb myproject_test.cpp -cflags -std=c++0x \n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-1510] Running: add_files -tb firmware/weights \n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-1510] Running: add_files -tb tb_data \n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-1510] Running: open_solution solution1 \n",
      "INFO: [HLS 200-10] Creating and opening solution '/home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1'.\n",
      "INFO: [HLS 200-1505] Using default flow_target 'vivado'\n",
      "Resolution: For help on HLS 200-1505 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2023.2%20English&url=ug1448-hls-guidance&resourceid=200-1505.html\n",
      "INFO: [HLS 200-1510] Running: config_array_partition -maximum_size 4096 \n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "ERROR: [HLS 200-642] The 'config_array_partition -maximum_size' command is not supported.\n",
      "INFO: [HLS 200-1510] Running: config_compile -name_max_length 80 \n",
      "INFO: [XFORM 203-1161] The maximum of name length is set to 80.\n",
      "INFO: [HLS 200-1510] Running: set_part xcvu13p-flga2577-2-e \n",
      "INFO: [HLS 200-1611] Setting target device to 'xcvu13p-flga2577-2-e'\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set to 80.\n",
      "INFO: [HLS 200-1510] Running: config_schedule -enable_dsp_full_reg=false \n",
      "INFO: [HLS 200-1510] Running: create_clock -period 10 -name default \n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.\n",
      "INFO: [HLS 200-1510] Running: set_clock_uncertainty 27% default \n",
      "INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 2.7ns.\n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [HLS 200-1510] Running: csynth_design \n",
      "INFO: [HLS 200-111] Finished File checks and directory preparation: CPU user time: 0.05 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.05 seconds; current allocated memory: 275.305 MB.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... \n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:40:63)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2023.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:40:67)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2023.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:48:67)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2023.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:48:71)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2023.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:56:67)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2023.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:56:71)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2023.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:64:67)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2023.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body (firmware/myproject.cpp:64:71)\n",
      "Resolution: For help on HLS 214-113 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2023.2%20English&url=ug1448-hls-guidance&resourceid=214-113.html\n",
      "WARNING: [HLS 200-471] Dataflow form checks found 8 issue(s) in file firmware/myproject.cpp\n",
      "Resolution: For help on HLS 200-471 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2023.2%20English&url=ug1448-hls-guidance&resourceid=200-471.html\n",
      "WARNING: [HLS 207-5292] unused parameter 'keep' (firmware/nnet_utils/nnet_helpers.h:285:99)\n",
      "WARNING: [HLS 207-5292] unused parameter 'data' (firmware/nnet_utils/nnet_code_gen.h:11:36)\n",
      "WARNING: [HLS 207-5292] unused parameter 'buffer' (firmware/nnet_utils/nnet_code_gen.h:12:36)\n",
      "WARNING: [HLS 207-5292] unused parameter 'partition' (firmware/nnet_utils/nnet_code_gen.h:13:44)\n",
      "WARNING: [HLS 207-5292] unused parameter 'data' (firmware/nnet_utils/nnet_code_gen.h:21:24)\n",
      "WARNING: [HLS 207-5292] unused parameter 'buffer' (firmware/nnet_utils/nnet_code_gen.h:22:24)\n",
      "WARNING: [HLS 207-5292] unused parameter 'partition' (firmware/nnet_utils/nnet_code_gen.h:23:32)\n",
      "INFO: [HLS 200-111] Finished Source Code Analysis and Preprocessing: CPU user time: 6.76 seconds. CPU system time: 0.58 seconds. Elapsed time: 7.38 seconds; current allocated memory: 280.801 MB.\n",
      "INFO: [HLS 200-777] Using interface defaults for 'Vivado' flow target.\n",
      "INFO: [HLS 200-1995] There were 7,956 instructions in the design after the 'Compile/Link' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 9,902 instructions in the design after the 'Unroll/Inline (step 1)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,855 instructions in the design after the 'Unroll/Inline (step 2)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,736 instructions in the design after the 'Unroll/Inline (step 3)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 3,366 instructions in the design after the 'Unroll/Inline (step 4)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 5,832 instructions in the design after the 'Array/Struct (step 1)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 5,177 instructions in the design after the 'Array/Struct (step 2)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 5,177 instructions in the design after the 'Array/Struct (step 3)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 5,606 instructions in the design after the 'Array/Struct (step 4)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 5,253 instructions in the design after the 'Array/Struct (step 5)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,990 instructions in the design after the 'Performance (step 1)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,990 instructions in the design after the 'Performance (step 2)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,990 instructions in the design after the 'Performance (step 3)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,990 instructions in the design after the 'Performance (step 4)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 4,996 instructions in the design after the 'HW Transforms (step 1)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 200-1995] There were 5,044 instructions in the design after the 'HW Transforms (step 2)' phase of compilation. See the Design Size Report for more details: /home/crchen/fbnet_deepcalo/rule4ml/notebooks/hls4ml_prj/myproject_prj/solution1/syn/report/csynth_design_size.rpt\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::dense_resource<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' (firmware/nnet_utils/nnet_dense_resource.h:250:9)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config2::weight_t*, config2::bias_t*)', Pragma conflict happens on 'INLINE' and 'FUNCTION_INSTANTIATE' pragmas: same function (firmware/nnet_utils/nnet_dense_resource.h:86:0)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::dense_resource<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>' (firmware/nnet_utils/nnet_dense_resource.h:250:9)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config4::weight_t*, config4::bias_t*)', Pragma conflict happens on 'INLINE' and 'FUNCTION_INSTANTIATE' pragmas: same function (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::dense_resource<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' (firmware/nnet_utils/nnet_dense_resource.h:250:9)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config6::weight_t*, config6::bias_t*)', Pragma conflict happens on 'INLINE' and 'FUNCTION_INSTANTIATE' pragmas: same function (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-415] Performing recursive inline in function 'nnet::dense_resource<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' (firmware/nnet_utils/nnet_dense_resource.h:250:9)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config8::weight_t*, config8::bias_t*)', Pragma conflict happens on 'INLINE' and 'FUNCTION_INSTANTIATE' pragmas: same function (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config2::weight_t*, config2::bias_t*)', Pragma conflict happens on 'INLINE' and DATAFLOW pragmas: Inline into dataflow region may break the canonical form (firmware/nnet_utils/nnet_dense.h:38:0)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config4::weight_t*, config4::bias_t*)', Pragma conflict happens on 'INLINE' and DATAFLOW pragmas: Inline into dataflow region may break the canonical form (firmware/nnet_utils/nnet_dense.h:38:0)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config6::weight_t*, config6::bias_t*)', Pragma conflict happens on 'INLINE' and DATAFLOW pragmas: Inline into dataflow region may break the canonical form (firmware/nnet_utils/nnet_dense.h:38:0)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::dense<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config8::weight_t*, config8::bias_t*)', Pragma conflict happens on 'INLINE' and DATAFLOW pragmas: Inline into dataflow region may break the canonical form (firmware/nnet_utils/nnet_dense.h:38:0)\n",
      "WARNING: [HLS 214-273] In function 'void nnet::softmax<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)', Pragma conflict happens on 'INLINE' and DATAFLOW pragmas: Inline into dataflow region may break the canonical form (firmware/nnet_utils/nnet_activation.h:379:0)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config2::weight_t*, config2::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:139:17)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config4::weight_t*, config4::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:56:17)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config6::weight_t*, config6::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:56:17)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config8::weight_t*, config8::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:56:17)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config2::weight_t*, config2::bias_t*)' into 'myproject(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)' (firmware/myproject.cpp:40:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config4::weight_t*, config4::bias_t*)' into 'myproject(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)' (firmware/myproject.cpp:48:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config6::weight_t*, config6::bias_t*)' into 'myproject(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)' (firmware/myproject.cpp:56:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config8::weight_t*, config8::bias_t*)' into 'myproject(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)' (firmware/myproject.cpp:64:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::softmax<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)' into 'myproject(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)' (firmware/myproject.cpp:66:5)\n",
      "INFO: [HLS 214-291] Loop 'VITIS_LOOP_266_3' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_activation.h:266:23)\n",
      "INFO: [HLS 214-291] Loop 'VITIS_LOOP_252_2' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_activation.h:252:23)\n",
      "INFO: [HLS 214-291] Loop 'VITIS_LOOP_243_1' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_activation.h:243:23)\n",
      "INFO: [HLS 214-291] Loop 'MultLoop' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_resource.h:52:9)\n",
      "INFO: [HLS 214-291] Loop 'VITIS_LOOP_43_1' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_activation.h:43:22)\n",
      "INFO: [HLS 214-291] Loop 'MultLoop' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_resource.h:136:9)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_266_3' (firmware/nnet_utils/nnet_activation.h:266:23) in function 'nnet::softmax_stable<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>' completely with a factor of 5 (firmware/nnet_utils/nnet_activation.h:216:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_252_2' (firmware/nnet_utils/nnet_activation.h:252:23) in function 'nnet::softmax_stable<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>' completely with a factor of 5 (firmware/nnet_utils/nnet_activation.h:216:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_243_1' (firmware/nnet_utils/nnet_activation.h:243:23) in function 'nnet::softmax_stable<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>' completely with a factor of 5 (firmware/nnet_utils/nnet_activation.h:216:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:77:5) in function 'nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:52:9) in function 'nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:37:5) in function 'nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_43_1' (firmware/nnet_utils/nnet_activation.h:43:22) in function 'nnet::relu<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, relu_config7>' completely with a factor of 32 (firmware/nnet_utils/nnet_activation.h:39:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:77:5) in function 'nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' completely with a factor of 32 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:52:9) in function 'nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' completely with a factor of 32 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:37:5) in function 'nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>' completely with a factor of 32 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_43_1' (firmware/nnet_utils/nnet_activation.h:43:22) in function 'nnet::relu<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, relu_config5>' completely with a factor of 32 (firmware/nnet_utils/nnet_activation.h:39:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:77:5) in function 'nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>' completely with a factor of 32 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:52:9) in function 'nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>' completely with a factor of 32 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:37:5) in function 'nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>' completely with a factor of 32 (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_43_1' (firmware/nnet_utils/nnet_activation.h:43:22) in function 'nnet::relu<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, relu_config3>' completely with a factor of 32 (firmware/nnet_utils/nnet_activation.h:39:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:156:5) in function 'nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 32 (firmware/nnet_utils/nnet_dense_resource.h:86:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:136:9) in function 'nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 16 (firmware/nnet_utils/nnet_dense_resource.h:86:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:108:5) in function 'nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 32 (firmware/nnet_utils/nnet_dense_resource.h:86:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_uint<1> >::value), ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>(config2::accum_t)' into 'void nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config2::weight_t*, config2::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:86:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_uint<1> >::value), ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>(config4::accum_t)' into 'void nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config4::weight_t*, config4::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_uint<1> >::value), ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>(config6::accum_t)' into 'void nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config6::weight_t*, config6::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_uint<1> >::value), ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>(config8::accum_t)' into 'void nnet::dense_resource_rf_leq_nin<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, config8::weight_t*, config8::bias_t*)' (firmware/nnet_utils/nnet_dense_resource.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> nnet::reduce<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> > >(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> const*, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> nnet::reduce<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> > >(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> const*, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >)' into 'ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> nnet::reduce<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> > >(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> const*, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> nnet::reduce<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> > >(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> const*, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> nnet::reduce<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> > >(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> const*, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >)' into 'ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> nnet::reduce<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 5, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> > >(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> const*, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> nnet::reduce<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 1, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> > >(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> const*, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >)' into 'ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> nnet::reduce<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 5, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> > >(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> const*, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>) (.62)' into 'ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> nnet::reduce<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 5, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> > >(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> const*, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>) (.33.42)' into 'ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> const*, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> const*, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >)' into 'ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> const*, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>) (.33.42)' into 'ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> const*, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 1, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> const*, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >)' into 'ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 5, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> const*, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>) (.33.42)' into 'ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 5, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> const*, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >)' (firmware/nnet_utils/nnet_common.h:36:0)\n",
      "INFO: [HLS 214-178] Inlining function 'ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> nnet::reduce<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, 5, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> > >(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> const*, nnet::Op_max<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0> >)' into 'void nnet::softmax_stable<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)' (firmware/nnet_utils/nnet_activation.h:216:0)\n",
      "INFO: [HLS 214-178] Inlining function 'unsigned int nnet::softmax_idx_from_real_val<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::softmax_stable<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)' (firmware/nnet_utils/nnet_activation.h:216:0)\n",
      "INFO: [HLS 214-178] Inlining function 'ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 5, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> const*, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >)' into 'void nnet::softmax_stable<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)' (firmware/nnet_utils/nnet_activation.h:216:0)\n",
      "INFO: [HLS 214-178] Inlining function 'unsigned int nnet::softmax_idx_from_real_val<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config9>(ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>)' into 'void nnet::softmax_stable<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>(ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>*)' (firmware/nnet_utils/nnet_activation.h:216:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b8': Complete partitioning on dimension 1. (firmware/weights/b8.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b6': Complete partitioning on dimension 1. (firmware/weights/b6.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b4': Complete partitioning on dimension 1. (firmware/weights/b4.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b2': Complete partitioning on dimension 1. (firmware/weights/b2.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'acc': Complete partitioning on dimension 1. (firmware/nnet_utils/nnet_dense_resource.h:104:29)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'exp_res': Complete partitioning on dimension 1. (firmware/nnet_utils/nnet_activation.h:249:36)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer2_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:38:11)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer3_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:42:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer4_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:46:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer5_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:50:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer6_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:54:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer7_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:58:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer8_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:62:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer9_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:10:0)\n",
      "INFO: [HLS 214-248] Applying array_reshape to 'w8': Block reshaping with factor 5 on dimension 1. (firmware/weights/w8.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_reshape to 'w6': Block reshaping with factor 32 on dimension 1. (firmware/weights/w6.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_reshape to 'w4': Block reshaping with factor 32 on dimension 1. (firmware/weights/w4.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_reshape to 'w2': Block reshaping with factor 16 on dimension 1. (firmware/weights/w2.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_reshape to 'input_1': Complete reshaping on dimension 1. (firmware/myproject.cpp:10:0)\n",
      "INFO: [HLS 200-111] Finished Compiling Optimization and Transform: CPU user time: 4.9 seconds. CPU system time: 0.63 seconds. Elapsed time: 7.36 seconds; current allocated memory: 283.191 MB.\n",
      "INFO: [HLS 200-111] Finished Checking Pragmas: CPU user time: 0 seconds. CPU system time: 0 seconds. Elapsed time: 0 seconds; current allocated memory: 283.191 MB.\n",
      "INFO: [HLS 200-10] Starting code transformations ...\n",
      "INFO: [HLS 200-111] Finished Standard Transforms: CPU user time: 0.23 seconds. CPU system time: 0 seconds. Elapsed time: 0.23 seconds; current allocated memory: 296.820 MB.\n",
      "INFO: [HLS 200-10] Checking synthesizability ...\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::softmax_stable<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>' (firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation.h:262) automatically.\n",
      "WARNING: [SYNCHK 200-23] firmware/nnet_utils/nnet_dense_resource.h:129: variable-indexed range selection may cause suboptimal QoR.\n",
      "INFO: [SYNCHK 200-10] 0 error(s), 1 warning(s).\n",
      "INFO: [HLS 200-111] Finished Checking Synthesizability: CPU user time: 0.17 seconds. CPU system time: 0 seconds. Elapsed time: 0.17 seconds; current allocated memory: 300.273 MB.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::softmax_stable<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>' (firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation.h:262) automatically.\n",
      "INFO: [XFORM 203-712] Applying dataflow to function 'myproject' (firmware/myproject.cpp:38:1), detected/extracted 8 process function(s): \n",
      "\t 'nnet::dense_resource<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>'\n",
      "\t 'nnet::relu<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, relu_config3>'\n",
      "\t 'nnet::dense_resource<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config4>'\n",
      "\t 'nnet::relu<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, relu_config5>'\n",
      "\t 'nnet::dense_resource<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config6>'\n",
      "\t 'nnet::relu<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, relu_config7>'\n",
      "\t 'nnet::dense_resource<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>'\n",
      "\t 'nnet::softmax_stable<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config9>'.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_dense_resource.h:130:9) to (firmware/nnet_utils/nnet_dense_resource.h:129:5) in function 'nnet::dense_resource_rf_gt_nin_rem0<ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 3, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>'... converting 33 basic blocks.\n",
      "INFO: [HLS 200-111] Finished Loop, function and other optimizations: CPU user time: 0.51 seconds. CPU system time: 0.02 seconds. Elapsed time: 0.52 seconds; current allocated memory: 324.562 MB.\n",
      "WARNING: [HLS 200-1992] Performance of loop 'ReuseLoop'(firmware/nnet_utils/nnet_dense_resource.h:43:5) in function 'dense_resource_rf_leq_nin<ap_fixed,ap_fixed<8,3,5,3,0>,config8>' can be improved with loop rewind inference if the loop is called from a region that can be executed in overlapped fashion such as a dataflow region or the top function.\n",
      "WARNING: [HLS 200-1992] Performance of loop 'ReuseLoop'(firmware/nnet_utils/nnet_dense_resource.h:43:5) in function 'dense_resource_rf_leq_nin<ap_fixed,ap_fixed<8,3,5,3,0>,config6>' can be improved with loop rewind inference if the loop is called from a region that can be executed in overlapped fashion such as a dataflow region or the top function.\n",
      "WARNING: [HLS 200-1992] Performance of loop 'ReuseLoop'(firmware/nnet_utils/nnet_dense_resource.h:43:5) in function 'dense_resource_rf_leq_nin<ap_fixed,ap_fixed<8,3,5,3,0>,config4>' can be improved with loop rewind inference if the loop is called from a region that can be executed in overlapped fashion such as a dataflow region or the top function.\n",
      "WARNING: [HLS 200-1992] Performance of loop 'ReuseLoop'(firmware/nnet_utils/nnet_dense_resource.h:129:5) in function 'dense_resource_rf_gt_nin_rem0<ap_fixed,ap_fixed<8,3,5,3,0>,config2>' can be improved with loop rewind inference if the loop is called from a region that can be executed in overlapped fashion such as a dataflow region or the top function.\n",
      "INFO: [XFORM 203-531] Rewinding loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:43) in function 'dense_resource_rf_leq_nin<ap_fixed,ap_fixed<8,3,5,3,0>,config8>'.\n",
      "INFO: [XFORM 203-531] Rewinding loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:43) in function 'dense_resource_rf_leq_nin<ap_fixed,ap_fixed<8,3,5,3,0>,config6>'.\n",
      "INFO: [XFORM 203-531] Rewinding loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:43) in function 'dense_resource_rf_leq_nin<ap_fixed,ap_fixed<8,3,5,3,0>,config4>'.\n",
      "INFO: [XFORM 203-531] Rewinding loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:129) in function 'dense_resource_rf_gt_nin_rem0<ap_fixed,ap_fixed<8,3,5,3,0>,config2>'.\n",
      "INFO: [HLS 200-111] Finished Architecture Synthesis: CPU user time: 0.24 seconds. CPU system time: 0.03 seconds. Elapsed time: 0.27 seconds; current allocated memory: 438.949 MB.\n",
      "INFO: [HLS 200-10] Starting hardware synthesis ...\n",
      "INFO: [HLS 200-10] Synthesizing 'myproject' ...\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource_rf_gt_nin_rem0<ap_fixed,ap_fixed<8,3,5,3,0>,config2>' to 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_8_3_5_3_0_config2_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, config2>' to 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config2_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, relu_config3>' to 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config3_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource_rf_leq_nin<ap_fixed,ap_fixed<8,3,5,3,0>,config4>' to 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config4_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, config4>' to 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config4_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, relu_config5>' to 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config5_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource_rf_leq_nin<ap_fixed,ap_fixed<8,3,5,3,0>,config6>' to 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config6_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, config6>' to 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config6_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, relu_config7>' to 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config7_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource_rf_leq_nin<ap_fixed,ap_fixed<8,3,5,3,0>,config8>' to 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config8_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_resource<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, config8>' to 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config8_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'softmax_stable<ap_fixed,ap_fixed<8,3,5,3,0>,softmax_config9>' to 'softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s'.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_8_3_5_3_0_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = 1, Final II = 1, Depth = 2, loop 'ReuseLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.26 seconds. CPU system time: 0.03 seconds. Elapsed time: 0.28 seconds; current allocated memory: 445.199 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.08 seconds; current allocated memory: 445.199 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 445.199 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 445.199 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'relu<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, relu_config3>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'relu<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, relu_config3>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.07 seconds. CPU system time: 0 seconds. Elapsed time: 0.07 seconds; current allocated memory: 445.199 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 445.199 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = 1, Final II = 1, Depth = 2, loop 'ReuseLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.29 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.29 seconds; current allocated memory: 450.031 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 450.031 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.05 seconds; current allocated memory: 450.031 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 450.031 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'relu<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, relu_config5>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'relu<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, relu_config5>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.07 seconds. CPU system time: 0 seconds. Elapsed time: 0.08 seconds; current allocated memory: 450.031 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 450.031 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = 1, Final II = 1, Depth = 2, loop 'ReuseLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.29 seconds. CPU system time: 0 seconds. Elapsed time: 0.3 seconds; current allocated memory: 455.109 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.03 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.04 seconds; current allocated memory: 455.109 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.05 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.05 seconds; current allocated memory: 455.109 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 455.109 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config7_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'relu<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, relu_config7>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'relu<ap_fixed<8, 3, 5, 3, 0>, ap_fixed<8, 3, 5, 3, 0>, relu_config7>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.07 seconds. CPU system time: 0 seconds. Elapsed time: 0.07 seconds; current allocated memory: 455.109 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 455.109 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = 1, Final II = 1, Depth = 2, loop 'ReuseLoop'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.13 seconds. CPU system time: 0 seconds. Elapsed time: 0.13 seconds; current allocated memory: 457.836 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 457.836 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.02 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.03 seconds; current allocated memory: 457.836 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 457.836 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'softmax_stable<ap_fixed,ap_fixed<8,3,5,3,0>,softmax_config9>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 3, function 'softmax_stable<ap_fixed,ap_fixed<8,3,5,3,0>,softmax_config9>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.06 seconds. CPU system time: 0 seconds. Elapsed time: 0.06 seconds; current allocated memory: 458.016 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 458.016 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.06 seconds. CPU system time: 0 seconds. Elapsed time: 0.06 seconds; current allocated memory: 459.355 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 459.559 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_8_3_5_3_0_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_8_3_5_3_0_config2_s_outidx_ROM_AUTO_1R' to 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_8_3_5_3_0_config2_s_outidx_RObkb' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_8_3_5_3_0_config2_s_w2_ROM_AUTO_1R' to 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_8_3_5_3_0_config2_s_w2_ROM_AUcud' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_8s_5s_13_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_8s_8s_13_1_1': 15 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_8_3_5_3_0_config2_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_8_3_5_3_0_config2_s_outidx_RObkb' using auto ROMs.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_gt_nin_rem0_ap_fixed_ap_fixed_8_3_5_3_0_config2_s_w2_ROM_AUcud' using auto ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.08 seconds. CPU system time: 0 seconds. Elapsed time: 0.09 seconds; current allocated memory: 466.039 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config2_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.08 seconds. CPU system time: 0 seconds. Elapsed time: 0.09 seconds; current allocated memory: 472.668 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config3_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 475.645 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_7ns_5s_12_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_8s_7ns_13_1_1': 31 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'sparsemux_65_5_7_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config4_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config4_s_w4_ROM_AUTO_1R' using auto ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.08 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.1 seconds; current allocated memory: 484.910 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config4_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.08 seconds. CPU system time: 0 seconds. Elapsed time: 0.09 seconds; current allocated memory: 492.047 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config5_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.04 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.07 seconds; current allocated memory: 495.977 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_7ns_5s_12_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_8s_7ns_13_1_1': 31 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'sparsemux_65_5_7_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config6_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config6_s_w6_ROM_AUTO_1R' using auto ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.09 seconds. CPU system time: 0 seconds. Elapsed time: 0.1 seconds; current allocated memory: 505.211 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config6_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.09 seconds. CPU system time: 0 seconds. Elapsed time: 0.1 seconds; current allocated memory: 512.324 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config7_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_relu_config7_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.05 seconds; current allocated memory: 516.293 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_7ns_5s_12_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_8s_7ns_13_1_1': 4 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'sparsemux_65_5_7_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config8_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_dense_resource_rf_leq_nin_ap_fixed_ap_fixed_8_3_5_3_0_config8_s_w8_ROM_AUTO_1R' using auto ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.06 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.07 seconds; current allocated memory: 520.629 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_resource_ap_fixed_8_3_5_3_0_ap_fixed_8_3_5_3_0_config8_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.06 seconds. CPU system time: 0 seconds. Elapsed time: 0.06 seconds; current allocated memory: 524.574 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s_exp_table_ROM_AUTO_1R' to 'softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s_exp_table_ROM_AUdEe' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s_invert_table_ROM_AUTO_1R' to 'softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s_invert_table_ROMeOg' due to the length limit 80\n",
      "INFO: [HLS 200-1030] Apply Unified Pipeline Control on module 'softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s' pipeline 'softmax_stable<ap_fixed,ap_fixed<8,3,5,3,0>,softmax_config9>' pipeline type 'function pipeline'\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_18s_16ns_23_1_1': 5 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s_exp_table_ROM_AUdEe' using auto ROMs.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_softmax_stable_ap_fixed_ap_fixed_8_3_5_3_0_softmax_config9_s_invert_table_ROMeOg' using auto ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.1 seconds; current allocated memory: 527.652 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/input_1' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer9_out_0' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer9_out_1' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer9_out_2' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer9_out_3' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer9_out_4' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on function 'myproject' to 'ap_ctrl_hs'.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'myproject'.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_1_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_2_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_3_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_4_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_5_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_6_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_7_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_8_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_9_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_10_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_11_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_12_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_13_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_14_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_15_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_16_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_17_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_18_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_19_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_20_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_21_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_22_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_23_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_24_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_25_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_26_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_27_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_28_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_29_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_30_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_31_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_1_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_2_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_3_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_4_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_5_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_6_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_7_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_8_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_9_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_10_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_11_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_12_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_13_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_14_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_15_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_16_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_17_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_18_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_19_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_20_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_21_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_22_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_23_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_24_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_25_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_26_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_27_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_28_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_29_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_30_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_31_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_1_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_2_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_3_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_4_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_5_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_6_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_7_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_8_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_9_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_10_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_11_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_12_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_13_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_14_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_15_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_16_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_17_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_18_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_19_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_20_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_21_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_22_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_23_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_24_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_25_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_26_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_27_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_28_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_29_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_30_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_31_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_1_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_2_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_3_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_4_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_5_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_6_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_7_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_8_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_9_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_10_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_11_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_12_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_13_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_14_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_15_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_16_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_17_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_18_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_19_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_20_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_21_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_22_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_23_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_24_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_25_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_26_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_27_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_28_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_29_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_30_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_31_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_1_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_2_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_3_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_4_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_5_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_6_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_7_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_8_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_9_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_10_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_11_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_12_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_13_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_14_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_15_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_16_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_17_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_18_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_19_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_20_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_21_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_22_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_23_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_24_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_25_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_26_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_27_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_28_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_29_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_30_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_31_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_1_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_2_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_3_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_4_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_5_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_6_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_7_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_8_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_9_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_10_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_11_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_12_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_13_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_14_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_15_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_16_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_17_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_18_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_19_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_20_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_21_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_22_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_23_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_24_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_25_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_26_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_27_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_28_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_29_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_30_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_31_U(myproject_fifo_w7_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_1_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_2_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_3_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_4_U(myproject_fifo_w8_d2_S)' using Shift Registers.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.46 seconds. CPU system time: 0.03 seconds. Elapsed time: 0.64 seconds; current allocated memory: 538.883 MB.\n",
      "INFO: [HLS 200-111] Finished Generating all RTL models: CPU user time: 0.34 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.36 seconds; current allocated memory: 548.000 MB.\n",
      "INFO: [HLS 200-111] Finished Updating report files: CPU user time: 0.47 seconds. CPU system time: 0.02 seconds. Elapsed time: 0.48 seconds; current allocated memory: 561.785 MB.\n",
      "INFO: [VHDL 208-304] Generating VHDL RTL for myproject.\n",
      "INFO: [VLOG 209-307] Generating Verilog RTL for myproject.\n",
      "INFO: [HLS 200-789] **** Estimated Fmax: 150.65 MHz\n",
      "INFO: [HLS 200-111] Finished Command csynth_design CPU user time: 16.79 seconds. CPU system time: 1.43 seconds. Elapsed time: 20.38 seconds; current allocated memory: 288.000 MB.\n",
      "***** C/RTL SYNTHESIS COMPLETED IN 0h0m20s *****\n",
      "INFO: [HLS 200-112] Total CPU user time: 18.3 seconds. Total CPU system time: 1.59 seconds. Total elapsed time: 21.99 seconds; peak allocated memory: 563.305 MB.\n",
      "INFO: [Common 17-206] Exiting vitis_hls at Sun Sep  1 20:41:32 2024...\n",
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "from data_gen.nn_synth import synthesize_keras_model\n",
    "\n",
    "input_size = 16\n",
    "inputs = Input(shape=(input_size,))\n",
    "x = Dense(32, activation=\"relu\")(inputs)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "outputs = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model_to_synthesize = keras.Model(inputs=inputs, outputs=outputs, name=\"Jet Classifier\")\n",
    "model_to_synthesize.build((None, input_size))\n",
    "\n",
    "synthesis_result = synthesize_keras_model(\n",
    "    model_to_synthesize,\n",
    "    board=\"pynq-z2\",\n",
    "    strategy=\"Resource\",\n",
    "    precision=\"ap_fixed<8, 3>\",\n",
    "    reuse_factor=32,\n",
    "    clock_period=\"10\",\n",
    "    io_type=\"io_parallel\",\n",
    "    project_dir=\"./hls4ml_prj\",\n",
    "    synth_uuid=None,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen.utils import save_to_json\n",
    "\n",
    "save_to_json(synthesis_result, \"./synthesis_result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta_data': {'uuid': '58e57381-6726-4f1a-aa4a-29dc8b813a6e',\n",
       "  'synthesis_start': '20240901-204110',\n",
       "  'synthesis_end': '20240901-204132',\n",
       "  'cpu': {'brand': '13th Gen Intel(R) Core(TM) i9-13900',\n",
       "   'architecture': 'X86_64',\n",
       "   'base_frequency': '3.8000 GHz',\n",
       "   'logical_count': 32,\n",
       "   'physical_count': 24}},\n",
       " 'model_config': [{'class_name': 'InputLayer',\n",
       "   'input_shape': ((None, 16),),\n",
       "   'output_shape': ((None, 16),),\n",
       "   'parameters': 0,\n",
       "   'trainable_parameters': 0,\n",
       "   'dtype': 'float32',\n",
       "   'reuse_factor': 32},\n",
       "  {'class_name': 'Dense',\n",
       "   'input_shape': (None, 16),\n",
       "   'output_shape': (None, 32),\n",
       "   'parameters': 544,\n",
       "   'trainable_parameters': 544,\n",
       "   'neurons': 32,\n",
       "   'use_bias': True,\n",
       "   'dtype': 'float32',\n",
       "   'reuse_factor': 32},\n",
       "  {'class_name': 'Activation',\n",
       "   'input_shape': (None, 32),\n",
       "   'output_shape': (None, 32),\n",
       "   'activation': 'relu',\n",
       "   'parameters': 0,\n",
       "   'trainable_parameters': 0,\n",
       "   'dtype': 'float32',\n",
       "   'reuse_factor': 32},\n",
       "  {'class_name': 'Dense',\n",
       "   'input_shape': (None, 32),\n",
       "   'output_shape': (None, 32),\n",
       "   'parameters': 1056,\n",
       "   'trainable_parameters': 1056,\n",
       "   'neurons': 32,\n",
       "   'use_bias': True,\n",
       "   'dtype': 'float32',\n",
       "   'reuse_factor': 32},\n",
       "  {'class_name': 'Activation',\n",
       "   'input_shape': (None, 32),\n",
       "   'output_shape': (None, 32),\n",
       "   'activation': 'relu',\n",
       "   'parameters': 0,\n",
       "   'trainable_parameters': 0,\n",
       "   'dtype': 'float32',\n",
       "   'reuse_factor': 32},\n",
       "  {'class_name': 'Dense',\n",
       "   'input_shape': (None, 32),\n",
       "   'output_shape': (None, 32),\n",
       "   'parameters': 1056,\n",
       "   'trainable_parameters': 1056,\n",
       "   'neurons': 32,\n",
       "   'use_bias': True,\n",
       "   'dtype': 'float32',\n",
       "   'reuse_factor': 32},\n",
       "  {'class_name': 'Activation',\n",
       "   'input_shape': (None, 32),\n",
       "   'output_shape': (None, 32),\n",
       "   'activation': 'relu',\n",
       "   'parameters': 0,\n",
       "   'trainable_parameters': 0,\n",
       "   'dtype': 'float32',\n",
       "   'reuse_factor': 32},\n",
       "  {'class_name': 'Dense',\n",
       "   'input_shape': (None, 32),\n",
       "   'output_shape': (None, 5),\n",
       "   'parameters': 165,\n",
       "   'trainable_parameters': 165,\n",
       "   'neurons': 5,\n",
       "   'use_bias': True,\n",
       "   'dtype': 'float32',\n",
       "   'reuse_factor': 32},\n",
       "  {'class_name': 'Activation',\n",
       "   'input_shape': (None, 5),\n",
       "   'output_shape': (None, 5),\n",
       "   'activation': 'softmax',\n",
       "   'parameters': 0,\n",
       "   'trainable_parameters': 0,\n",
       "   'dtype': 'float32',\n",
       "   'reuse_factor': 32}],\n",
       " 'hls_config': {'model': {'precision': 'ap_fixed<8, 3>',\n",
       "   'reuse_factor': 32,\n",
       "   'strategy': 'Resource',\n",
       "   'bram_factor': 1000000000,\n",
       "   'trace_output': False},\n",
       "  'clock_period': 10.0,\n",
       "  'io_type': 'io_parallel',\n",
       "  'board': 'pynq-z2'},\n",
       " 'resource_report': {'bram': 4.0,\n",
       "  'dsp': 5.0,\n",
       "  'ff': 8969.0,\n",
       "  'lut': 20867.0,\n",
       "  'uram': 0.0},\n",
       " 'latency_report': {'cycles_min': 141.0,\n",
       "  'cycles_max': 145.0,\n",
       "  'target_clock': 10.0,\n",
       "  'estimated_clock': 6.638}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthesis_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 PyTorch Model <a class=\"anchor\" id=\"torch-synth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data_gen.nn_synth import synthesize_torch_model\n",
    "from data_gen.utils import save_to_json\n",
    "\n",
    "model_to_synthesize = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, 1),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "synthesis_result = synthesize_torch_model(\n",
    "    model_to_synthesize,\n",
    "    board=\"zcu102\",\n",
    "    strategy=\"Latency\",\n",
    "    precision=\"ap_fixed<8, 3>\",\n",
    "    reuse_factor=32,\n",
    "    clock_period=\"10\",\n",
    "    io_type=\"io_parallel\",\n",
    "    project_dir=\"./hls4ml_prj\",\n",
    "    synth_uuid=None,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "save_to_json(synthesis_result, \"./synthesis_result.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Parallel Synthesis <a class=\"anchor\" id=\"parallel-synth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Randomly Generated Networks <a class=\"anchor\" id=\"random-synth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "from data_gen.nn_gen import GeneratorSettings, generate_fc_network\n",
    "from data_gen.nn_synth import (\n",
    "    SynthSettings,\n",
    "    synthesize_keras_model,\n",
    "    parallel_generative_synthesis,\n",
    ")\n",
    "\n",
    "from data_gen.utils import IntRange, Power2Range, save_to_json\n",
    "\n",
    "gen_settings = GeneratorSettings(\n",
    "    input_range=Power2Range(16, 32),\n",
    "    layer_range=IntRange(2, 3),\n",
    "    neuron_range=Power2Range(16, 32),\n",
    "    output_range=IntRange(1, 20),\n",
    "    activations=[\"relu\"],\n",
    ")\n",
    "synth_settings = SynthSettings(\n",
    "    reuse_range=Power2Range(32, 64),\n",
    "    precisions=[\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\"],\n",
    "    strategies=[\"Resource\"],\n",
    ")\n",
    "\n",
    "n_procs = 3\n",
    "with Pool(n_procs) as p:\n",
    "    result = p.map_async(\n",
    "        parallel_generative_synthesis,\n",
    "        [\n",
    "            {\n",
    "                \"job_id\": f\"{proc}\",\n",
    "                \"n_models\": 10,\n",
    "                \"project_dir\": \"./projects\",\n",
    "                \"prj_overwrite\": False,\n",
    "                \"save_path\": \"./\",\n",
    "                \"rng_seed\": 0,\n",
    "                \"gen_function\": generate_fc_network,  # Keras only currently\n",
    "                \"gen_settings\": gen_settings,\n",
    "                \"synth_function\": synthesize_keras_model,\n",
    "                \"synth_settings\": synth_settings,\n",
    "            }\n",
    "            for proc in range(1, n_procs + 1)\n",
    "        ],\n",
    "    )\n",
    "    while not result.ready():\n",
    "        time.sleep(1)\n",
    "    result = result.get()\n",
    "    p.terminate()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Prediction Models <a class=\"anchor\" id=\"train-models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Parsing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Reading from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.parsers.data_parser import (\n",
    "    read_from_json,\n",
    "    ParsedDataFilter,\n",
    "    get_global_data,\n",
    "    get_sequential_data,\n",
    "    to_dataframe,\n",
    ")\n",
    "\n",
    "from rule4ml.parsers.data_parser import (\n",
    "    default_board_map,\n",
    "    default_strategy_map,\n",
    "    default_layer_type_map,\n",
    ")\n",
    "\n",
    "data_filter = ParsedDataFilter(\n",
    "    max_output_size=200,\n",
    ")\n",
    "\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "json_data = read_from_json(\n",
    "    os.path.join(base_path, \"datasets/fcnn_dataset_15000.json\"),\n",
    "    data_filter,\n",
    ")\n",
    "\n",
    "meta_data, global_inputs, targets = get_global_data(json_data)\n",
    "sequential_inputs = get_sequential_data(json_data)\n",
    "\n",
    "# Ordinal encoding of categorical inputs\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}\n",
    "\n",
    "df = to_dataframe(\n",
    "    meta_data=meta_data,\n",
    "    global_inputs=global_inputs,\n",
    "    sequential_inputs=sequential_inputs,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    targets=targets,\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sequential_inputs\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "seed_num = 1337\n",
    "np.random.seed(seed_num)\n",
    "keras.utils.set_random_seed(seed_num)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.05, random_state=seed_num)\n",
    "print(f\"Train Dataframe: {train_df.shape}\")\n",
    "print(f\"Test Dataframe: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training MLPs <a class=\"anchor\" id=\"train-mlps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Data Preprocessing <a class=\"anchor\" id=\"mlp-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = [  # Selecting input features\n",
    "    \"strategy\",\n",
    "    \"board\",\n",
    "    # \"precision\",\n",
    "    \"bit_width\",\n",
    "    # \"integer_bits\",\n",
    "    # \"fractional_bits\",\n",
    "    \"reuse_mean\",\n",
    "    # \"dense_count\",\n",
    "    # \"batchnormalization_count\",\n",
    "    # \"add_count\",\n",
    "    # \"concatenate_count\",\n",
    "    # \"dropout_count\",\n",
    "    # \"relu_count\",\n",
    "    # \"sigmoid_count\",\n",
    "    # \"tanh_count\",\n",
    "    # \"softmax_count\",\n",
    "    # \"dense_parameters_mean\",\n",
    "    # \"dense_inputs_mean\",\n",
    "    # \"dense_outputs_mean\",\n",
    "    # \"dense_reuse_mean\",\n",
    "    \"dense_inputs_mean\",\n",
    "    # \"dense_inputs_min\",\n",
    "    # \"dense_inputs_min_idx\",\n",
    "    # \"dense_inputs_max\",\n",
    "    # \"dense_inputs_max_idx\",\n",
    "    \"dense_outputs_mean\",\n",
    "    # \"dense_outputs_min\",\n",
    "    # \"dense_outputs_min_idx\",\n",
    "    # \"dense_outputs_max\",\n",
    "    # \"dense_outputs_max_idx\",\n",
    "    \"dense_parameters_mean\",\n",
    "    # \"dense_parameters_min\",\n",
    "    # \"dense_parameters_min_idx\",\n",
    "    # \"dense_parameters_max\",\n",
    "    # \"dense_parameters_max_idx\",\n",
    "    \"dense_reuse_mean\",\n",
    "    # \"dense_reuse_min\",\n",
    "    # \"dense_reuse_min_idx\",\n",
    "    # \"dense_reuse_max\",\n",
    "    # \"dense_reuse_max_idx\",\n",
    "    \"dense_count\",\n",
    "    \"batchnormalization_inputs_mean\",\n",
    "    # \"batchnormalization_inputs_min\",\n",
    "    # \"batchnormalization_inputs_min_idx\",\n",
    "    # \"batchnormalization_inputs_max\",\n",
    "    # \"batchnormalization_inputs_max_idx\",\n",
    "    \"batchnormalization_outputs_mean\",\n",
    "    # \"batchnormalization_outputs_min\",\n",
    "    # \"batchnormalization_outputs_min_idx\",\n",
    "    # \"batchnormalization_outputs_max\",\n",
    "    # \"batchnormalization_outputs_max_idx\",\n",
    "    \"batchnormalization_parameters_mean\",\n",
    "    # \"batchnormalization_parameters_min\",\n",
    "    # \"batchnormalization_parameters_min_idx\",\n",
    "    # \"batchnormalization_parameters_max\",\n",
    "    # \"batchnormalization_parameters_max_idx\",\n",
    "    \"batchnormalization_count\",\n",
    "    # \"add_inputs_mean\",\n",
    "    # \"add_inputs_min\",\n",
    "    # \"add_inputs_min_idx\",\n",
    "    # \"add_inputs_max\",\n",
    "    # \"add_inputs_max_idx\",\n",
    "    # \"add_outputs_mean\",\n",
    "    # \"add_outputs_min\",\n",
    "    # \"add_outputs_min_idx\",\n",
    "    # \"add_outputs_max\",\n",
    "    # \"add_outputs_max_idx\",\n",
    "    \"add_count\",\n",
    "    # \"concatenate_inputs_mean\",\n",
    "    # \"concatenate_inputs_min\",\n",
    "    # \"concatenate_inputs_min_idx\",\n",
    "    # \"concatenate_inputs_max\",\n",
    "    # \"concatenate_inputs_max_idx\",\n",
    "    # \"concatenate_outputs_mean\",\n",
    "    # \"concatenate_outputs_min\",\n",
    "    # \"concatenate_outputs_min_idx\",\n",
    "    # \"concatenate_outputs_max\",\n",
    "    # \"concatenate_outputs_max_idx\",\n",
    "    \"concatenate_count\",\n",
    "    # \"dropout_inputs_mean\",\n",
    "    # \"dropout_inputs_min\",\n",
    "    # \"dropout_inputs_min_idx\",\n",
    "    # \"dropout_inputs_max\",\n",
    "    # \"dropout_inputs_max_idx\",\n",
    "    # \"dropout_outputs_mean\",\n",
    "    # \"dropout_outputs_min\",\n",
    "    # \"dropout_outputs_min_idx\",\n",
    "    # \"dropout_outputs_max\",\n",
    "    # \"dropout_outputs_max_idx\",\n",
    "    \"dropout_count\",\n",
    "    # \"relu_inputs_mean\",\n",
    "    # \"relu_inputs_min\",\n",
    "    # \"relu_inputs_min_idx\",\n",
    "    # \"relu_inputs_max\",\n",
    "    # \"relu_inputs_max_idx\",\n",
    "    # \"relu_outputs_mean\",\n",
    "    # \"relu_outputs_min\",\n",
    "    # \"relu_outputs_min_idx\",\n",
    "    # \"relu_outputs_max\",\n",
    "    # \"relu_outputs_max_idx\",\n",
    "    \"relu_count\",\n",
    "    # \"sigmoid_inputs_mean\",\n",
    "    # \"sigmoid_inputs_min\",\n",
    "    # \"sigmoid_inputs_min_idx\",\n",
    "    # \"sigmoid_inputs_max\",\n",
    "    # \"sigmoid_inputs_max_idx\",\n",
    "    # \"sigmoid_outputs_mean\",\n",
    "    # \"sigmoid_outputs_min\",\n",
    "    # \"sigmoid_outputs_min_idx\",\n",
    "    # \"sigmoid_outputs_max\",\n",
    "    # \"sigmoid_outputs_max_idx\",\n",
    "    \"sigmoid_count\",\n",
    "    # \"tanh_inputs_mean\",\n",
    "    # \"tanh_inputs_min\",\n",
    "    # \"tanh_inputs_min_idx\",\n",
    "    # \"tanh_inputs_max\",\n",
    "    # \"tanh_inputs_max_idx\",\n",
    "    # \"tanh_outputs_mean\",\n",
    "    # \"tanh_outputs_min\",\n",
    "    # \"tanh_outputs_min_idx\",\n",
    "    # \"tanh_outputs_max\",\n",
    "    # \"tanh_outputs_max_idx\",\n",
    "    \"tanh_count\",\n",
    "    \"softmax_inputs_mean\",\n",
    "    # \"softmax_inputs_min\",\n",
    "    # \"softmax_inputs_min_idx\",\n",
    "    # \"softmax_inputs_max\",\n",
    "    # \"softmax_inputs_max_idx\",\n",
    "    \"softmax_outputs_mean\",\n",
    "    # \"softmax_outputs_min\",\n",
    "    # \"softmax_outputs_min_idx\",\n",
    "    # \"softmax_outputs_max\",\n",
    "    # \"softmax_outputs_max_idx\",\n",
    "    \"softmax_count\",\n",
    "    # \"total_mult\",\n",
    "    # \"total_add\",\n",
    "    # \"total_logical\",\n",
    "    # \"total_lookup\",\n",
    "]\n",
    "\n",
    "train_inputs_df = train_df[feature_labels].copy()\n",
    "test_inputs_df = test_df[feature_labels].copy()\n",
    "\n",
    "train_inputs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = [\"lut\"]\n",
    "\n",
    "train_targets_df = train_df[target_labels].copy()\n",
    "test_targets_df = test_df[target_labels].copy()\n",
    "\n",
    "train_targets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Building and Training <a class=\"anchor\" id=\"fit-mlps\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import MLPSettings, ModelWrapper\n",
    "\n",
    "input_shape = (None, len(train_inputs_df.columns))\n",
    "output_shape = (None, len(train_targets_df.columns))\n",
    "\n",
    "bram_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[32, 16, 32],\n",
    "    dense_layers=[256, 256, 256, 64, 32, 64, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "dsp_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 32, 32],\n",
    "    dense_layers=[256, 16, 32, 32, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "ff_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 16, 32],\n",
    "    dense_layers=[64, 128, 64, 256, 32],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "lut_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 16, 32, 32],\n",
    "    dense_layers=[64, 128, 128, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "cycles_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[32, 16, 64],\n",
    "    dense_layers=[256, 32, 32, 32, 256, 128, 128, 32, 16, 16, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "\n",
    "mlp_settings = lut_mlp_settings\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "model_wrapper.build_mlp_model(\n",
    "    mlp_settings,\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    categorical_maps=global_categorical_maps,\n",
    "    model_name=f\"{'-'.join([x.upper() for x in target_labels])}_MLP\",\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import TrainSettings\n",
    "from rule4ml.models.metrics import parametric_smape, parametric_r2\n",
    "\n",
    "smape = parametric_smape(0, \"-\".join([x.upper() for x in target_labels]))\n",
    "r2 = parametric_r2(0, \"-\".join([x.upper() for x in target_labels]))\n",
    "\n",
    "bram_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "dsp_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "ff_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "lut_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "cycles_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-3,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "\n",
    "train_settings = cycles_train_settings\n",
    "\n",
    "model_wrapper.build_dataset(\n",
    "    train_inputs_df,\n",
    "    train_targets_df,\n",
    "    train_settings.batch_size,\n",
    "    val_ratio=0.15,\n",
    "    train_repeats=10,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = os.path.join(\"./logs\", f\"{model_wrapper.model.name}_{start_time}\")\n",
    "checkpoint_dir = os.path.join(\"./checkpoints\", f\"{model_wrapper.model.name}_{start_time}\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_file = os.path.join(checkpoint_dir, f\"{'-'.join(target_labels)}_best.weights.h5\")\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq=\"epoch\",\n",
    "    embeddings_freq=1,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_file,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        return lr * np.exp(-0.2)\n",
    "    return lr\n",
    "\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "callbacks = [\n",
    "    tensorboard_callback,\n",
    "    checkpoint_callback,\n",
    "    # lr_callback\n",
    "]\n",
    "\n",
    "fit_history = model_wrapper.fit(train_settings, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training Transformers <a class=\"anchor\" id=\"train-transformers\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Data Preprocessing <a class=\"anchor\" id=\"transformer-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_labels = [\n",
    "    \"strategy\",\n",
    "    \"board\",\n",
    "    \"bit_width\",\n",
    "    \"reuse_mean\",\n",
    "    \"dense_inputs_mean\",\n",
    "    \"dense_outputs_mean\",\n",
    "    \"dense_parameters_mean\",\n",
    "    \"dense_reuse_mean\",\n",
    "    \"dense_count\",\n",
    "    \"batchnormalization_inputs_mean\",\n",
    "    \"batchnormalization_outputs_mean\",\n",
    "    \"batchnormalization_parameters_mean\",\n",
    "    \"batchnormalization_count\",\n",
    "    \"add_count\",\n",
    "    \"concatenate_count\",\n",
    "    \"dropout_count\",\n",
    "    \"relu_count\",\n",
    "    \"sigmoid_count\",\n",
    "    \"tanh_count\",\n",
    "    \"softmax_inputs_mean\",\n",
    "    \"softmax_outputs_mean\",\n",
    "    \"softmax_count\",\n",
    "]\n",
    "sequential_feature_labels = [\n",
    "    \"layer_type\",\n",
    "    \"layer_input_size\",\n",
    "    \"layer_output_size\",\n",
    "    \"layer_parameter_count\",\n",
    "    \"layer_reuse\",\n",
    "]\n",
    "\n",
    "feature_labels = global_feature_labels\n",
    "if len(sequential_feature_labels) > 0:\n",
    "    feature_labels += [\"sequential_inputs\"]\n",
    "inputs_df = df[feature_labels].copy()\n",
    "inputs_df[\"sequential_inputs\"] = inputs_df[\"sequential_inputs\"].apply(\n",
    "    lambda x: x[sequential_feature_labels]\n",
    ")\n",
    "\n",
    "inputs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_df[\"sequential_inputs\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = [\"lut\"]\n",
    "targets_df = df[target_labels].copy()\n",
    "targets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Building and Training <a class=\"anchor\" id=\"fit-transformers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import TransformerSettings, ModelWrapper\n",
    "\n",
    "global_input_shape = (None, len(inputs_df.columns) - 1)  # not considering \"sequential_inputs\"\n",
    "sequential_input_shape = (None, len(inputs_df[\"sequential_inputs\"].iloc[0].columns))\n",
    "output_shape = (None, len(targets_df.columns))\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "model_wrapper.build_transformer_model(\n",
    "    TransformerSettings(\n",
    "        global_dense_layers=[128, 192, 192],\n",
    "        seq_dense_layers=[32, 64, 96],\n",
    "        global_numerical_dense_layers=[16, 8],\n",
    "        seq_numerical_dense_layers=[32],\n",
    "        num_blocks=1,\n",
    "        num_heads=8,\n",
    "        ff_dim=256,\n",
    "        output_dim=192,\n",
    "        dropout_rate=0.2,\n",
    "        embedding_outputs=[24, 24, 16, 8],\n",
    "        dense_layers=[192, 128, 64, 32, 64, 128, 256, 32],\n",
    "        dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    ),\n",
    "    global_input_shape=global_input_shape,\n",
    "    sequential_input_shape=sequential_input_shape,\n",
    "    output_shape=output_shape,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    model_name=f\"{'-'.join([x.upper() for x in target_labels])}_Transformer\",\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import TrainSettings\n",
    "\n",
    "train_settings = TrainSettings(num_epochs=50)\n",
    "model_wrapper.build_dataset(\n",
    "    inputs_df,\n",
    "    targets_df,\n",
    "    train_settings.batch_size,\n",
    "    val_ratio=0.15,\n",
    "    train_repeats=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = os.path.join(\"./logs\", f\"{model_wrapper.model.name}_{start_time}\")\n",
    "checkpoint_dir = os.path.join(\"./checkpoints\", f\"{model_wrapper.model.name}_{start_time}\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_file = os.path.join(checkpoint_dir, f\"{'-'.join(target_labels)}_best.weights.h5\")\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq=\"epoch\",\n",
    "    embeddings_freq=1,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_file,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        return lr * np.exp(-0.2)\n",
    "    return lr\n",
    "\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "callbacks = [\n",
    "    tensorboard_callback,\n",
    "    checkpoint_callback,\n",
    "    # lr_callback\n",
    "]\n",
    "\n",
    "fit_history = model_wrapper.fit(train_settings, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Finetuning (Optional) <a class=\"anchor\" id=\"finetune\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Finetuning an MLP <a class=\"anchor\" id=\"finetune-mlp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.tuning import Searcher\n",
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "target_labels = [\"lut\"]\n",
    "\n",
    "train_targets_df = train_df[target_labels].copy()\n",
    "test_targets_df = test_df[target_labels].copy()\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "searcher = Searcher(model_wrapper)\n",
    "searcher.mlp_search(\n",
    "    train_inputs_df,\n",
    "    train_targets_df,\n",
    "    global_categorical_maps,\n",
    "    directory=\"./mlp_search\",\n",
    "    verbose=1,\n",
    ")\n",
    "searcher.tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Loading and Retraining <a class=\"anchor\" id=\"load-tuner\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.tuning import Searcher\n",
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "searcher = Searcher(model_wrapper)\n",
    "searcher.load_tuner(\n",
    "    train_inputs_df,\n",
    "    train_targets_df,\n",
    "    global_categorical_maps,\n",
    "    \"./mlp_search\",\n",
    "    \"20240715-090815\",\n",
    ")\n",
    "searcher.tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import TrainSettings\n",
    "\n",
    "model_wrapper = searcher.model_wrapper\n",
    "model_wrapper.fit(searcher.train_settings, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing Prediction Models <a class=\"anchor\" id=\"test-models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Benchmark Networks <a class=\"anchor\" id=\"benchmark-test\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Add,\n",
    "    Input,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Flatten,\n",
    ")\n",
    "\n",
    "\n",
    "def get_test_model(name):\n",
    "    model = None\n",
    "    if name == \"jet\":\n",
    "        input_size = 16\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(5, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"quarks\":\n",
    "        input_size = 10\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(1, use_bias=True)(x)\n",
    "        outputs = Activation(\"sigmoid\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"anomaly\":\n",
    "        input_size = 128\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(8, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(4, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(128, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(4, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(128, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"bipc\":\n",
    "        input_size = 36\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(36, use_bias=False)(inputs)\n",
    "\n",
    "        y = Activation(\"relu\")(x)\n",
    "        for i in range(5):\n",
    "            y = Dense(36, use_bias=False)(y)\n",
    "            y = Add()([x, y])\n",
    "            y = Activation(\"relu\")(y)\n",
    "        outputs = y\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"cookie\":\n",
    "        input_size = 512\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(4, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(5, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"mnist\":\n",
    "        input_size = 784\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(16, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(10, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"automlp\":\n",
    "        input_size = 7\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(12, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(16, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(12, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(2, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"particle\":\n",
    "        input_size = 14\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(3, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"custom1\":\n",
    "        input_size = 16\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(64, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(10, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"custom2\":\n",
    "        input_size = 128\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(16, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(64, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(64, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(50, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"custom3\":\n",
    "\n",
    "        def residual_block(x, units):\n",
    "            y = Dense(units)(x)\n",
    "            y = BatchNormalization()(y)\n",
    "            y = Activation(\"relu\")(y)\n",
    "\n",
    "            y = Dense(units)(y)\n",
    "            y = BatchNormalization()(y)\n",
    "\n",
    "            if x.shape[-1] == units:\n",
    "                y = Add()([x, y])\n",
    "            else:\n",
    "                x = Dense(units)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                y = Add()([x, y])\n",
    "\n",
    "            y = Activation(\"relu\")(y)\n",
    "            return y\n",
    "\n",
    "        input_size = 64\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "        x = residual_block(x, units=32)\n",
    "        x = residual_block(x, units=32)\n",
    "\n",
    "        x = Dense(10)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"conv2d-nopool\":\n",
    "        input_size = (16, 16, 1)\n",
    "        inputs = Input(input_size)\n",
    "        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2D(4, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(2, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper, MultiModelEstimator\n",
    "import itertools\n",
    "\n",
    "hls_configs = [\n",
    "    {\n",
    "        \"model\": {\n",
    "            \"precision\": \"ap_fixed<8, 3>\",\n",
    "            \"reuse_factor\": 32,\n",
    "            \"strategy\": strategy,\n",
    "            \"bram_factor\": 1000000000,\n",
    "            \"trace_output\": False,\n",
    "        },\n",
    "        \"clock_period\": 10.0,\n",
    "        \"io_type\": \"io_parallel\",\n",
    "        \"board\": board,\n",
    "    }\n",
    "    for board, strategy in itertools.product([\"pynq-z2\", \"zcu102\"], [\"Latency\", \"Resource\"])\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"jet\",\n",
    "    \"quarks\",\n",
    "    \"anomaly\",\n",
    "    \"bipc\",\n",
    "    \"cookie\",\n",
    "    \"mnist\",\n",
    "    \"automlp\",\n",
    "    \"particle\",\n",
    "    \"custom1\",\n",
    "    \"custom2\",\n",
    "    \"custom3\",\n",
    "]\n",
    "models = [get_test_model(name) for name in model_names]\n",
    "\n",
    "target_labels = [\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\"]\n",
    "\n",
    "estimator = MultiModelEstimator()\n",
    "for label in target_labels:\n",
    "    model_wrapper = ModelWrapper()\n",
    "    model_wrapper.load(\n",
    "        f\"./models/best_{label.upper()}_MLP_config.json\",\n",
    "        f\"./models/best_{label.upper()}_MLP.weights.h5\",\n",
    "    )\n",
    "    estimator.add_model_wrapper(model_wrapper)\n",
    "\n",
    "prediction_df = estimator.predict(models, hls_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.sort_values([\"Board\", \"Strategy\", \"Reuse Factor\"]).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Plots <a class=\"anchor\" id=\"plots\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Box Plots <a class=\"anchor\" id=\"box-plots\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from rule4ml.parsers.data_parser import (\n",
    "    read_from_json,\n",
    "    ParsedDataFilter,\n",
    "    get_global_data,\n",
    "    get_sequential_data,\n",
    "    to_dataframe,\n",
    ")\n",
    "\n",
    "from rule4ml.parsers.data_parser import (\n",
    "    default_board_map,\n",
    "    default_strategy_map,\n",
    "    default_layer_type_map,\n",
    ")\n",
    "\n",
    "data_filter = ParsedDataFilter(\n",
    "    max_output_size=200,\n",
    ")\n",
    "\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "json_data = read_from_json(\n",
    "    os.path.join(base_path, \"datasets/fcnn_dataset_15000.json\"),\n",
    "    data_filter,\n",
    ")\n",
    "\n",
    "meta_data, global_inputs, targets = get_global_data(json_data)\n",
    "sequential_inputs = get_sequential_data(json_data)\n",
    "\n",
    "# Ordinal encoding of categorical inputs\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}\n",
    "\n",
    "df = to_dataframe(\n",
    "    meta_data=meta_data,\n",
    "    global_inputs=global_inputs,\n",
    "    sequential_inputs=sequential_inputs,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    targets=targets,\n",
    ")\n",
    "\n",
    "seed_num = 1337\n",
    "np.random.seed(seed_num)\n",
    "keras.utils.set_random_seed(seed_num)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.05, random_state=seed_num)\n",
    "\n",
    "feature_labels = [\n",
    "    \"strategy\",\n",
    "    \"board\",\n",
    "    \"bit_width\",\n",
    "    \"reuse_mean\",\n",
    "    \"dense_inputs_mean\",\n",
    "    \"dense_inputs_min\",\n",
    "    \"dense_inputs_min_idx\",\n",
    "    \"dense_inputs_max\",\n",
    "    \"dense_inputs_max_idx\",\n",
    "    \"dense_outputs_mean\",\n",
    "    \"dense_outputs_min\",\n",
    "    \"dense_outputs_min_idx\",\n",
    "    \"dense_outputs_max\",\n",
    "    \"dense_outputs_max_idx\",\n",
    "    \"dense_parameters_mean\",\n",
    "    \"dense_parameters_min\",\n",
    "    \"dense_parameters_min_idx\",\n",
    "    \"dense_parameters_max\",\n",
    "    \"dense_parameters_max_idx\",\n",
    "    \"dense_reuse_mean\",\n",
    "    \"dense_reuse_min\",\n",
    "    \"dense_reuse_min_idx\",\n",
    "    \"dense_reuse_max\",\n",
    "    \"dense_reuse_max_idx\",\n",
    "    \"dense_count\",\n",
    "    \"batchnormalization_inputs_mean\",\n",
    "    \"batchnormalization_outputs_mean\",\n",
    "    \"batchnormalization_parameters_mean\",\n",
    "    \"batchnormalization_count\",\n",
    "    \"add_count\",\n",
    "    \"concatenate_count\",\n",
    "    \"dropout_count\",\n",
    "    \"relu_count\",\n",
    "    \"sigmoid_count\",\n",
    "    \"tanh_count\",\n",
    "    \"softmax_inputs_mean\",\n",
    "    \"softmax_outputs_mean\",\n",
    "    \"softmax_count\",\n",
    "]\n",
    "\n",
    "test_inputs_df = test_df[feature_labels].copy()\n",
    "print(f\"Test Inputs: {test_inputs_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "prediction_labels = [\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\"]\n",
    "test_targets_df = test_df[prediction_labels].copy()\n",
    "\n",
    "wrappers = []\n",
    "prediction_errors = []\n",
    "for label in prediction_labels:\n",
    "    wrapper = ModelWrapper()\n",
    "    wrapper.load(\n",
    "        f\"./models/best_{label.upper()}_MLP_config.json\",\n",
    "        f\"./models/best_{label.upper()}_MLP.weights.h5\",\n",
    "    )\n",
    "    wrappers.append(wrapper)\n",
    "\n",
    "    pred = wrapper.predict_from_df(test_inputs_df).squeeze()\n",
    "    gn = test_targets_df[label].values\n",
    "\n",
    "    prediction_errors.append(np.abs(gn - pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(prediction_errors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "fig, axis = plt.subplots(2, 2, figsize=(12, 8), width_ratios=[3, 1])\n",
    "axis = np.reshape(axis, -1)\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.4)\n",
    "\n",
    "flier_ax, box_ax = axis[0], axis[2]\n",
    "\n",
    "iqr_weight = 1.5\n",
    "\n",
    "resources_errors = prediction_errors[:4]\n",
    "resources_labels = prediction_labels[:4]\n",
    "\n",
    "threshold = 10.0\n",
    "below_threshold = []\n",
    "for errors in np.asarray(resources_errors):\n",
    "    below_threshold.append(np.sum(errors < threshold) / len(errors))\n",
    "print(f\"Resources below {threshold}%: {below_threshold}\")\n",
    "print(f\"Resources Mean: {np.mean(below_threshold)}\")\n",
    "\n",
    "bplot = box_ax.boxplot(\n",
    "    resources_errors,\n",
    "    whis=iqr_weight,\n",
    "    tick_labels=[x.upper() for x in resources_labels],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "fliers = flier_ax.boxplot(\n",
    "    resources_errors,\n",
    "    whis=iqr_weight,\n",
    "    tick_labels=[\"\" for x in resources_labels],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "\n",
    "colors = [\"pink\", \"yellow\", \"lightgreen\", \"lightblue\", \"FFA500\"]\n",
    "for patch, color in zip(bplot[\"boxes\"], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "box_ax.set_ylim(-1, 30)\n",
    "flier_ax.set_ylim(30, 200)\n",
    "\n",
    "box_ax.yaxis.grid(True)\n",
    "box_ax.spines.top.set_visible(False)\n",
    "box_ax.xaxis.tick_bottom()\n",
    "box_ax.set_yticks([0, 5, 10, 15, 20, 25, 30])\n",
    "\n",
    "flier_ax.yaxis.grid(True)\n",
    "flier_ax.spines.bottom.set_visible(False)\n",
    "flier_ax.xaxis.tick_top()\n",
    "flier_ax.set_yticks([30, 50, 75, 100, 125, 150, 175, 200])\n",
    "\n",
    "d = 0.5\n",
    "kwargs = dict(\n",
    "    marker=[(-1, -d), (1, d)],\n",
    "    markersize=12,\n",
    "    linestyle=\"none\",\n",
    "    color=\"k\",\n",
    "    mec=\"k\",\n",
    "    mew=1,\n",
    "    clip_on=False,\n",
    ")\n",
    "flier_ax.plot([0, 1], [0, 0], transform=flier_ax.transAxes, **kwargs)\n",
    "box_ax.plot([0, 1], [1, 1], transform=box_ax.transAxes, **kwargs)\n",
    "\n",
    "median_line = Line2D([0], [0], color=\"orange\", linestyle=\"--\", linewidth=1.5, label=\"Median\")\n",
    "mean_line = Line2D([0], [0], color=\"green\", linestyle=\"--\", linewidth=1.5, label=\"Mean\")\n",
    "\n",
    "handles = [median_line, mean_line]\n",
    "labels = [\"Median\", \"Mean\"]\n",
    "\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    bbox_to_anchor=[0.9, 1],\n",
    "    # loc=\"upper left\",\n",
    "    loc=\"upper right\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "ytext = fig.text(0.06, 0.5, \"Error (%)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "suptitle = fig.suptitle(\"Prediction Errors - Boxplots\", fontsize=20, y=0.95)\n",
    "\n",
    "latency_flier_ax, latency_box_ax = axis[1], axis[3]\n",
    "\n",
    "iqr_weight = 1.5\n",
    "\n",
    "latency_errors = [prediction_errors[4]]\n",
    "latency_labels = [prediction_labels[4]]\n",
    "\n",
    "threshold = 100.0\n",
    "below_threshold = []\n",
    "for errors in np.asarray(latency_errors):\n",
    "    below_threshold.append(np.sum(errors < threshold) / len(errors))\n",
    "print(f\"Latency below {threshold} cycles: {below_threshold}\")\n",
    "\n",
    "latency_bplot = latency_box_ax.boxplot(\n",
    "    latency_errors,\n",
    "    whis=iqr_weight,\n",
    "    widths=0.33,\n",
    "    tick_labels=[\"Cycles\"],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "latency_fliers = latency_flier_ax.boxplot(\n",
    "    latency_errors,\n",
    "    whis=iqr_weight,\n",
    "    widths=0.33,\n",
    "    tick_labels=[\"\" for x in latency_labels],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "\n",
    "colors = [\"lightblue\"]\n",
    "for patch, color in zip(latency_bplot[\"boxes\"], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "latency_box_ax.set_ylim(-10, 200)\n",
    "latency_flier_ax.set_ylim(200, 650)\n",
    "\n",
    "latency_box_ax.yaxis.grid(True)\n",
    "latency_box_ax.spines.top.set_visible(False)\n",
    "latency_box_ax.xaxis.tick_bottom()\n",
    "latency_box_ax.set_yticks(np.arange(0, 225, 25))\n",
    "\n",
    "latency_flier_ax.yaxis.grid(True)\n",
    "latency_flier_ax.spines.bottom.set_visible(False)\n",
    "latency_flier_ax.xaxis.tick_top()\n",
    "latency_flier_ax.set_yticks(np.arange(200, 700, 100))\n",
    "\n",
    "d = 0.5\n",
    "kwargs = dict(\n",
    "    marker=[(-1, -d), (1, d)],\n",
    "    markersize=12,\n",
    "    linestyle=\"none\",\n",
    "    color=\"k\",\n",
    "    mec=\"k\",\n",
    "    mew=1,\n",
    "    clip_on=False,\n",
    ")\n",
    "latency_flier_ax.plot([0, 1], [0, 0], transform=latency_flier_ax.transAxes, **kwargs)\n",
    "latency_box_ax.plot([0, 1], [1, 1], transform=latency_box_ax.transAxes, **kwargs)\n",
    "\n",
    "latency_ytext = fig.text(0.66, 0.5, \"Error (Cycles)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "resource_caption = fig.text(0.355, 0.04, \"(a)\", va=\"center\", size=18)\n",
    "latency_caption = fig.text(0.808, 0.04, \"(b)\", va=\"center\", size=18)\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/box_plot_merged.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, ytext, suptitle, latency_ytext, resource_caption, latency_caption),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Bar Plots <a class=\"anchor\" id=\"bar-plots\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper, MultiModelEstimator\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "prediction_labels = [\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\"]\n",
    "\n",
    "model_names = [\n",
    "    \"jet\",\n",
    "    \"quarks\",\n",
    "    \"anomaly\",\n",
    "    \"bipc\",\n",
    "    \"cookie\",\n",
    "    \"mnist\",\n",
    "    \"automlp\",\n",
    "    \"particle\",\n",
    "    \"custom1\",\n",
    "    \"custom2\",\n",
    "    \"custom3\",\n",
    "]\n",
    "test_models = [get_test_model(name) for name in model_names]\n",
    "\n",
    "hls_configs = [\n",
    "    {\n",
    "        \"model\": {\n",
    "            \"precision\": precision,\n",
    "            \"reuse_factor\": reuse,\n",
    "            \"strategy\": strategy,\n",
    "            \"bram_factor\": 1000000000,\n",
    "            \"trace_output\": False,\n",
    "        },\n",
    "        \"clock_period\": 10.0,\n",
    "        \"io_type\": \"io_parallel\",\n",
    "        \"board\": board,\n",
    "    }\n",
    "    for board, strategy, precision, reuse in itertools.product(\n",
    "        [\"pynq-z2\", \"zcu102\"],\n",
    "        [\"Latency\", \"Resource\"],\n",
    "        [\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\", \"ap_fixed<16, 6>\"],\n",
    "        [1, 2, 4, 8, 16, 32, 64],\n",
    "    )\n",
    "]\n",
    "\n",
    "estimator = MultiModelEstimator()\n",
    "estimator.load_default_models()\n",
    "predictions = []\n",
    "\n",
    "prediction_df = estimator.predict(test_models, hls_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[\"BRAM\"] = prediction_df[\"BRAM\"].apply(lambda x: min(x, 200.0))\n",
    "prediction_df[\"DSP\"] = prediction_df[\"DSP\"].apply(lambda x: min(x, 200.0))\n",
    "prediction_df[\"FF\"] = prediction_df[\"FF\"].apply(lambda x: min(x, 200.0))\n",
    "prediction_df[\"LUT\"] = prediction_df[\"LUT\"].apply(lambda x: min(x, 200.0))\n",
    "\n",
    "precision_order = [\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\", \"ap_fixed<16, 6>\"]\n",
    "prediction_df[\"Precision\"] = pd.Categorical(\n",
    "    prediction_df[\"Precision\"], categories=precision_order, ordered=True\n",
    ")\n",
    "\n",
    "prediction_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.parsers.data_parser import (\n",
    "    read_from_json,\n",
    "    get_global_data,\n",
    "    get_sequential_data,\n",
    "    to_dataframe,\n",
    "    default_strategy_map,\n",
    "    default_board_map,\n",
    "    default_layer_type_map,\n",
    ")\n",
    "\n",
    "benchmark_data = read_from_json(\"../datasets/benchmark_data.json\")\n",
    "\n",
    "benchmark_meta_data, benchmark_global_inputs, benchmark_targets = get_global_data(benchmark_data)\n",
    "benchmark_sequential_inputs = get_sequential_data(benchmark_data)\n",
    "\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}\n",
    "\n",
    "benchmark_df = to_dataframe(\n",
    "    meta_data=benchmark_meta_data,\n",
    "    global_inputs=benchmark_global_inputs,\n",
    "    sequential_inputs=benchmark_sequential_inputs,\n",
    "    global_categorical_maps={},\n",
    "    sequential_categorical_maps={},\n",
    "    targets=benchmark_targets,\n",
    ")\n",
    "benchmark_gn_df = benchmark_df[\n",
    "    [\n",
    "        \"model_name\",\n",
    "        \"board\",\n",
    "        \"strategy\",\n",
    "        \"precision\",\n",
    "        \"global_reuse\",\n",
    "        \"bram\",\n",
    "        \"dsp\",\n",
    "        \"ff\",\n",
    "        \"lut\",\n",
    "        \"cycles\",\n",
    "    ]\n",
    "].copy()\n",
    "benchmark_gn_df = benchmark_gn_df.rename(\n",
    "    {\n",
    "        \"model_name\": \"Model\",\n",
    "        \"board\": \"Board\",\n",
    "        \"strategy\": \"Strategy\",\n",
    "        \"precision\": \"Precision\",\n",
    "        \"global_reuse\": \"Reuse Factor\",\n",
    "        \"bram\": \"BRAM\",\n",
    "        \"dsp\": \"DSP\",\n",
    "        \"ff\": \"FF\",\n",
    "        \"lut\": \"LUT\",\n",
    "        \"cycles\": \"CYCLES\",\n",
    "    },\n",
    "    axis=1,\n",
    ")\n",
    "benchmark_gn_df.loc[benchmark_gn_df[\"Strategy\"] == \"latency\", \"Strategy\"] = \"Latency\"\n",
    "benchmark_gn_df.loc[benchmark_gn_df[\"Strategy\"] == \"resource\", \"Strategy\"] = \"Resource\"\n",
    "\n",
    "benchmark_gn_df[\"BRAM\"] = benchmark_gn_df[\"BRAM\"].apply(lambda x: min(x, 200.0))\n",
    "benchmark_gn_df[\"DSP\"] = benchmark_gn_df[\"DSP\"].apply(lambda x: min(x, 200.0))\n",
    "benchmark_gn_df[\"FF\"] = benchmark_gn_df[\"FF\"].apply(lambda x: min(x, 200.0))\n",
    "benchmark_gn_df[\"LUT\"] = benchmark_gn_df[\"LUT\"].apply(lambda x: min(x, 200.0))\n",
    "\n",
    "precision_order = [\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\", \"ap_fixed<16, 6>\"]\n",
    "benchmark_gn_df[\"Precision\"] = pd.Categorical(\n",
    "    benchmark_gn_df[\"Precision\"], categories=precision_order, ordered=True\n",
    ")\n",
    "\n",
    "benchmark_gn_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_grouped_mean = (\n",
    "    benchmark_gn_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[\n",
    "        [\n",
    "            \"BRAM\",\n",
    "            \"DSP\",\n",
    "            \"FF\",\n",
    "            \"LUT\",\n",
    "            # \"CYCLES\"\n",
    "        ]\n",
    "    ]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "prediction_grouped_mean = (\n",
    "    prediction_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[\n",
    "        [\n",
    "            \"BRAM\",\n",
    "            \"DSP\",\n",
    "            \"FF\",\n",
    "            \"LUT\",\n",
    "            # \"CYCLES\"\n",
    "        ]\n",
    "    ]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    gn_grouped_mean,\n",
    "    prediction_grouped_mean,\n",
    "    on=(\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"),\n",
    "    suffixes=(\" (G)\", \" (P)\"),\n",
    ")\n",
    "\n",
    "merged_df = merged_df[\n",
    "    [\n",
    "        \"BRAM (G)\",\n",
    "        \"BRAM (P)\",\n",
    "        \"DSP (G)\",\n",
    "        \"DSP (P)\",\n",
    "        \"FF (G)\",\n",
    "        \"FF (P)\",\n",
    "        \"LUT (G)\",\n",
    "        \"LUT (P)\",\n",
    "        # \"CYCLES (G)\", \"CYCLES (P)\",\n",
    "    ]\n",
    "]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rule4ml.parsers.utils import fixed_precision_to_bit_width\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "grouped = merged_df.xs((\"pynq-z2\",), level=[\"Board\"]).groupby([\"Precision\", \"Strategy\"])\n",
    "\n",
    "n_groups = len(grouped)\n",
    "n_cols = 2\n",
    "n_rows = 3\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, dpi=300, figsize=(16, 10), squeeze=False, sharex=True, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "width = 0.11\n",
    "colors = [\"#008000\", \"#FF5964\", \"#17BEBB\", \"#FFA500\"]\n",
    "reuse_factors = prediction_df[\"Reuse Factor\"].unique()\n",
    "num_resources = 4\n",
    "resource_gap = 0\n",
    "\n",
    "total_width = num_resources * (2 * width + resource_gap) - resource_gap\n",
    "start = np.arange(1, len(reuse_factors) + 1) - total_width / 2\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for ax, ((precision, strategy), df) in zip(axes, grouped):\n",
    "    for i, (col_gn, col_pred) in enumerate(zip(df.columns[::2], df.columns[1::2])):\n",
    "        gn_vals = df[col_gn]\n",
    "        pred_vals = df[col_pred]\n",
    "\n",
    "        resource_indices = start + i * (2 * width + resource_gap)\n",
    "\n",
    "        for j, reuse_factor in enumerate(reuse_factors):\n",
    "            gn_label = \"\"\n",
    "            pred_label = \"\"\n",
    "            if j == 0:\n",
    "                gn_label = f\"{col_gn}\"\n",
    "                pred_label = f\"{col_pred}\"\n",
    "\n",
    "            ax.bar(\n",
    "                resource_indices[j] - width / 2,\n",
    "                gn_vals[j],\n",
    "                width,\n",
    "                label=gn_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            ax.bar(\n",
    "                resource_indices[j] + width / 2,\n",
    "                pred_vals[j],\n",
    "                width,\n",
    "                label=pred_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "                hatch=\"///\",\n",
    "            )\n",
    "\n",
    "    total_bits, fraction_bits = fixed_precision_to_bit_width(precision)\n",
    "\n",
    "    ax.set_title(f\"{strategy}, {total_bits}-bit width\")\n",
    "    ax.set_xticks(start + (num_resources - 1) * (width + resource_gap / 2))\n",
    "    ax.set_xticklabels(reuse_factors, rotation=45)\n",
    "\n",
    "    # if col_idx == 0:\n",
    "    #     ax.set_ylabel(\"Utilization (%)\")\n",
    "\n",
    "    # if row_idx == n_rows - 1:\n",
    "    #     ax.set_xlabel(\"Reuse Factor\")\n",
    "\n",
    "    col_idx += 1\n",
    "    if col_idx == n_cols:\n",
    "        row_idx += 1\n",
    "        col_idx = 0\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Resources\",\n",
    "    bbox_to_anchor=[0.3, 1.03],\n",
    "    loc=\"upper left\",\n",
    "    # loc=\"upper right\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "xtext = fig.text(0.5, 0.035, \"Reuse Factor\", ha=\"center\", size=18)\n",
    "ytext = fig.text(0.07, 0.5, \"Utilization (%)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "suptitle = fig.suptitle(\"Pynq-Z2: Resource Utilization Trends\", fontsize=20, y=1.075)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.275, wspace=0.125)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/pynq_avg_bars.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, xtext, ytext, suptitle),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "grouped = merged_df.xs((\"zcu102\",), level=[\"Board\"]).groupby([\"Precision\", \"Strategy\"])\n",
    "\n",
    "n_groups = len(grouped)\n",
    "n_cols = 2\n",
    "n_rows = 3\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, dpi=300, figsize=(16, 10), squeeze=False, sharex=True, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "width = 0.11\n",
    "colors = [\"#008000\", \"#FF5964\", \"#17BEBB\", \"#FFA500\"]\n",
    "reuse_factors = prediction_df[\"Reuse Factor\"].unique()\n",
    "num_resources = 4\n",
    "resource_gap = 0\n",
    "\n",
    "total_width = num_resources * (2 * width + resource_gap) - resource_gap\n",
    "start = np.arange(1, len(reuse_factors) + 1) - total_width / 2\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for ax, ((precision, strategy), df) in zip(axes, grouped):\n",
    "    for i, (col_gn, col_pred) in enumerate(zip(df.columns[::2], df.columns[1::2])):\n",
    "        gn_vals = df[col_gn]\n",
    "        pred_vals = df[col_pred]\n",
    "\n",
    "        resource_indices = start + i * (2 * width + resource_gap)\n",
    "\n",
    "        for j, reuse_factor in enumerate(reuse_factors):\n",
    "            gn_label = \"\"\n",
    "            pred_label = \"\"\n",
    "            if j == 0:\n",
    "                gn_label = f\"{col_gn}\"\n",
    "                pred_label = f\"{col_pred}\"\n",
    "\n",
    "            ax.bar(\n",
    "                resource_indices[j] - width / 2,\n",
    "                gn_vals[j],\n",
    "                width,\n",
    "                label=gn_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            ax.bar(\n",
    "                resource_indices[j] + width / 2,\n",
    "                pred_vals[j],\n",
    "                width,\n",
    "                label=pred_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "                hatch=\"///\",\n",
    "            )\n",
    "\n",
    "    total_bits, fraction_bits = fixed_precision_to_bit_width(precision)\n",
    "\n",
    "    ax.set_title(f\"{strategy}, {total_bits}-bit width\")\n",
    "    ax.set_xticks(start + (num_resources - 1) * (width + resource_gap / 2))\n",
    "    ax.set_xticklabels(reuse_factors, rotation=45)\n",
    "\n",
    "    col_idx += 1\n",
    "    if col_idx == n_cols:\n",
    "        row_idx += 1\n",
    "        col_idx = 0\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Resources\",\n",
    "    bbox_to_anchor=[0.3, 1.03],\n",
    "    loc=\"upper left\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "xtext = fig.text(0.5, 0.035, \"Reuse Factor\", ha=\"center\", size=18)\n",
    "ytext = fig.text(0.07, 0.5, \"Utilization (%)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "suptitle = fig.suptitle(\"ZCU102: Resource Utilization Trends\", fontsize=20, y=1.075)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.275, wspace=0.125)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/zcu_avg_bars.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, xtext, ytext, suptitle),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_grouped_mean = (\n",
    "    benchmark_gn_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[[\"CYCLES\"]]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "prediction_grouped_mean = (\n",
    "    prediction_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[[\"CYCLES\"]]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    gn_grouped_mean,\n",
    "    prediction_grouped_mean,\n",
    "    on=(\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"),\n",
    "    suffixes=(\" (G)\", \" (P)\"),\n",
    ")\n",
    "\n",
    "merged_df = merged_df[\n",
    "    [\n",
    "        \"CYCLES (G)\",\n",
    "        \"CYCLES (P)\",\n",
    "    ]\n",
    "]\n",
    "merged_df.head()\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "grouped = merged_df.groupby([\"Board\", \"Strategy\", \"Precision\"])\n",
    "\n",
    "n_groups = len(grouped)\n",
    "n_cols = 3\n",
    "n_rows = (n_groups // n_cols) + (n_groups % n_cols > 0)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, dpi=300, figsize=(16, 10), squeeze=False, sharex=True, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "width = 0.35\n",
    "colors = [\"#17BEBB\", \"#FFA500\"]\n",
    "reuse_factors = prediction_df[\"Reuse Factor\"].unique()\n",
    "num_resources = 1\n",
    "resource_gap = 0\n",
    "\n",
    "total_width = num_resources * (2 * width + resource_gap) - resource_gap\n",
    "start = np.arange(1, len(reuse_factors) + 1) - total_width / 2\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for ax, ((board, strategy, precision), df) in zip(axes, grouped):\n",
    "    for i, (col_gn, col_pred) in enumerate(zip(df.columns[::2], df.columns[1::2])):\n",
    "        gn_vals = df[col_gn]\n",
    "        pred_vals = df[col_pred]\n",
    "\n",
    "        resource_indices = start + i * (2 * width + resource_gap)\n",
    "\n",
    "        for j, reuse_factor in enumerate(reuse_factors):\n",
    "            gn_label = \"\"\n",
    "            pred_label = \"\"\n",
    "            if j == 0:\n",
    "                gn_label = f\"{col_gn}\"\n",
    "                pred_label = f\"{col_pred}\"\n",
    "\n",
    "            ax.bar(\n",
    "                resource_indices[j] - width / 2,\n",
    "                gn_vals[j],\n",
    "                width,\n",
    "                label=gn_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            ax.bar(\n",
    "                resource_indices[j] + width / 2,\n",
    "                pred_vals[j],\n",
    "                width,\n",
    "                label=pred_label,\n",
    "                color=colors[i % len(colors) + 1],\n",
    "                edgecolor=\"black\",\n",
    "                hatch=\"///\",\n",
    "            )\n",
    "\n",
    "    total_bits, fraction_bits = fixed_precision_to_bit_width(precision)\n",
    "\n",
    "    ax.set_title(f\"{board}, {strategy}, {total_bits}-bit width\")\n",
    "    ax.set_xticks(start + (num_resources - 1) * (width + resource_gap / 2))\n",
    "    ax.set_xticklabels(reuse_factors, rotation=45)\n",
    "\n",
    "    col_idx += 1\n",
    "    if col_idx == n_cols:\n",
    "        row_idx += 1\n",
    "        col_idx = 0\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legends = fig.legend(handles, labels, bbox_to_anchor=[0.8, 1], loc=\"upper left\")\n",
    "\n",
    "xtext = fig.text(0.5, 0.05, \"Reuse Factor\", ha=\"center\", size=18)\n",
    "ytext = fig.text(0.07, 0.5, \"Cycles\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "fig.suptitle(\"Clock Cycle Trends\", fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/cycles_avg_bars.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, xtext, ytext),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optionally force tensorflow on CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [General Usage](#general-usage)\n",
    "    1. [Basic Usage](#basic-usage)\n",
    "    2. [Advanced Usage](#advanced-usage)\n",
    "2. [Data Generation](#data-gen)\n",
    "    1. [Model Synthesis](#model-synth)\n",
    "        1. [Keras Synthesis](#keras-synth)\n",
    "        2. [PyTorch Synthesis](#torch-synth)\n",
    "    2. [Parallel Synthesis](#parallel-synth)\n",
    "        1. [Randomly Generated Networks](#random-synth)\n",
    "3. [Training Prediction Models](#train-models)\n",
    "    1. [Parsing Datasets](#parse-data)\n",
    "        1. [Reading from JSON](#read-json)\n",
    "    2. [Training MLPs](#train-mlps)\n",
    "        1. [Data Preprocessing](#mlp-data)\n",
    "        2. [Building & Training](#fit-mlps)\n",
    "    3. [Training Transformers](#train-transformers)\n",
    "        1. [Data Preprocessing](#transformer-data)\n",
    "        2. [Building & Training](#fit-transformers)\n",
    "    4. [Finetuning (Optional)](#finetune)\n",
    "        1. [Finetuning an MLP](#finetune-mlp)\n",
    "        2. [Loading and Retraining](#load-tuner)\n",
    "4. [Testing Prediction Models](#test-models)\n",
    "    1. [Benchmark Networks](#benchmark-test)\n",
    "    2. [Plots](#plots)\n",
    "        1. [Box Plots](#box-plots)\n",
    "        2. [Bar Plots](#bar-plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General Usage <a class=\"anchor\" id=\"general-usage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Basic Usage <a class=\"anchor\" id=\"basic-usage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Activation\n",
    "import keras\n",
    "\n",
    "# Example of a keras Model to predict\n",
    "input_size = 16\n",
    "inputs = Input(shape=(input_size,))\n",
    "x = Dense(32, activation=\"relu\")(inputs)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "outputs = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model_to_predict = keras.Model(inputs=inputs, outputs=outputs, name=\"Jet Classifier\")\n",
    "model_to_predict.build((None, input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import MultiModelEstimator\n",
    "\n",
    "# Load default estimator\n",
    "estimator = MultiModelEstimator()\n",
    "estimator.load_default_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiModelEstimator predictions are formatted as a DataFrame\n",
    "prediction_df = estimator.predict(model_to_predict)\n",
    "\n",
    "# each row is unique in the groupby, mean() is only called to convert DataFrameGroupBy into a nicely organized DataFrame\n",
    "if not prediction_df.empty:\n",
    "    prediction_df = prediction_df.groupby(\n",
    "        [\"Model\", \"Board\", \"Strategy\", \"Precision\", \"Reuse Factor\"], observed=True\n",
    "    ).mean()\n",
    "\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_html(\"keras_example.html\")\n",
    "\n",
    "# prediction_df.to_latex(\"keras_example.tex\")\n",
    "# prediction_df.to_csv(\"keras_example.csv\")\n",
    "# prediction_df.to_json(\"keras_example.json\")\n",
    "# prediction_df.to_xml(\"keras_example.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Advanced Usage <a class=\"anchor\" id=\"advanced-usage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "import torch\n",
    "\n",
    "models_to_predict = []\n",
    "\n",
    "\n",
    "# Example of a subclassed PyTorch model\n",
    "class MyTopQuarks(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyTopQuarks, self).__init__()\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(10, 32)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dense2 = torch.nn.Linear(32, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        outputs = self.sigmoid(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "models_to_predict.append(MyTopQuarks())\n",
    "\n",
    "# Example of a keras Sequential model\n",
    "input_size = 16\n",
    "model_to_predict = keras.Sequential(\n",
    "    layers=[\n",
    "        keras.layers.Input(shape=(input_size,)),\n",
    "        keras.layers.Dense(32, use_bias=True),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        keras.layers.Dense(32, use_bias=True),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        keras.layers.Dense(32, use_bias=True),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        keras.layers.Dense(5, use_bias=True),\n",
    "        keras.layers.Activation(\"softmax\"),\n",
    "    ],\n",
    "    name=\"Jet Classifier\",\n",
    ")\n",
    "model_to_predict.build((None, input_size))\n",
    "\n",
    "models_to_predict.append(model_to_predict)\n",
    "\n",
    "hls_configs = [\n",
    "    {\n",
    "        \"model\": {\n",
    "            \"precision\": \"ap_fixed<8, 3>\",\n",
    "            \"reuse_factor\": 32,\n",
    "            \"strategy\": strategy,\n",
    "        },\n",
    "        \"board\": board,\n",
    "    }\n",
    "    for board, strategy in itertools.product([\"pynq-z2\", \"zcu102\"], [\"Latency\", \"Resource\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "lut_model_wrapper = ModelWrapper()\n",
    "lut_model_wrapper.load(\"./models/best_LUT_MLP_config.json\", \"./models/best_LUT_MLP.weights.h5\")\n",
    "\n",
    "lut_model_wrapper.predict(models_to_predict, hls_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import MultiModelEstimator\n",
    "\n",
    "estimator = MultiModelEstimator()\n",
    "estimator.add_model_wrapper(lut_model_wrapper)\n",
    "\n",
    "estimator.predict(models_to_predict, hls_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Generation <a class=\"anchor\" id=\"data-gen\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Specify Vivado path\n",
    "os.environ[\"PATH\"] = \"/opt/Xilinx/Vivado/2019.1/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\", \"data_gen\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model Synthesis <a class=\"anchor\" id=\"model-synth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Keras Model <a class=\"anchor\" id=\"keras-synth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "from data_gen.nn_synth import synthesize_keras_model\n",
    "\n",
    "input_size = 16\n",
    "inputs = Input(shape=(input_size,))\n",
    "x = Dense(32, activation=\"relu\")(inputs)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "outputs = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model_to_synthesize = keras.Model(inputs=inputs, outputs=outputs, name=\"Jet Classifier\")\n",
    "model_to_synthesize.build((None, input_size))\n",
    "\n",
    "synthesis_result = synthesize_keras_model(\n",
    "    model_to_synthesize,\n",
    "    board=\"pynq-z2\",\n",
    "    strategy=\"Resource\",\n",
    "    precision=\"ap_fixed<8, 3>\",\n",
    "    reuse_factor=32,\n",
    "    clock_period=\"10\",\n",
    "    io_type=\"io_parallel\",\n",
    "    project_dir=\"./hls4ml_prj\",\n",
    "    synth_uuid=None,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen.utils import save_to_json\n",
    "\n",
    "save_to_json(synthesis_result, \"./synthesis_result.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 PyTorch Model <a class=\"anchor\" id=\"torch-synth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data_gen.nn_synth import synthesize_torch_model\n",
    "from data_gen.utils import save_to_json\n",
    "\n",
    "model_to_synthesize = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, 1),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "synthesis_result = synthesize_torch_model(\n",
    "    model_to_synthesize,\n",
    "    board=\"zcu102\",\n",
    "    strategy=\"Latency\",\n",
    "    precision=\"ap_fixed<8, 3>\",\n",
    "    reuse_factor=32,\n",
    "    clock_period=\"10\",\n",
    "    io_type=\"io_parallel\",\n",
    "    project_dir=\"./hls4ml_prj\",\n",
    "    synth_uuid=None,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "save_to_json(synthesis_result, \"./synthesis_result.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Parallel Synthesis <a class=\"anchor\" id=\"parallel-synth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Randomly Generated Networks <a class=\"anchor\" id=\"random-synth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "from data_gen.nn_gen import GeneratorSettings, generate_fc_network\n",
    "from data_gen.nn_synth import (\n",
    "    SynthSettings,\n",
    "    synthesize_keras_model,\n",
    "    parallel_generative_synthesis,\n",
    ")\n",
    "\n",
    "from data_gen.utils import IntRange, Power2Range, save_to_json\n",
    "\n",
    "gen_settings = GeneratorSettings(\n",
    "    input_range=Power2Range(16, 32),\n",
    "    layer_range=IntRange(2, 3),\n",
    "    neuron_range=Power2Range(16, 32),\n",
    "    output_range=IntRange(1, 20),\n",
    "    activations=[\"relu\"],\n",
    ")\n",
    "synth_settings = SynthSettings(\n",
    "    reuse_range=Power2Range(32, 64),\n",
    "    precisions=[\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\"],\n",
    "    strategies=[\"Resource\"],\n",
    ")\n",
    "\n",
    "n_procs = 3\n",
    "with Pool(n_procs) as p:\n",
    "    result = p.map_async(\n",
    "        parallel_generative_synthesis,\n",
    "        [\n",
    "            {\n",
    "                \"job_id\": f\"{proc}\",\n",
    "                \"n_models\": 10,\n",
    "                \"project_dir\": \"./projects\",\n",
    "                \"prj_overwrite\": False,\n",
    "                \"save_path\": \"./\",\n",
    "                \"rng_seed\": 0,\n",
    "                \"gen_function\": generate_fc_network,  # Keras only currently\n",
    "                \"gen_settings\": gen_settings,\n",
    "                \"synth_function\": synthesize_keras_model,\n",
    "                \"synth_settings\": synth_settings,\n",
    "            }\n",
    "            for proc in range(1, n_procs + 1)\n",
    "        ],\n",
    "    )\n",
    "    while not result.ready():\n",
    "        time.sleep(1)\n",
    "    result = result.get()\n",
    "    p.terminate()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Prediction Models <a class=\"anchor\" id=\"train-models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Parsing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Reading from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.parsers.data_parser import (\n",
    "    read_from_json,\n",
    "    ParsedDataFilter,\n",
    "    get_global_data,\n",
    "    get_sequential_data,\n",
    "    to_dataframe,\n",
    ")\n",
    "\n",
    "from rule4ml.parsers.data_parser import (\n",
    "    default_board_map,\n",
    "    default_strategy_map,\n",
    "    default_layer_type_map,\n",
    ")\n",
    "\n",
    "data_filter = ParsedDataFilter(\n",
    "    max_output_size=200,\n",
    ")\n",
    "\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "json_data = read_from_json(\n",
    "    os.path.join(base_path, \"datasets/fcnn_dataset_15000.json\"),\n",
    "    data_filter,\n",
    ")\n",
    "\n",
    "meta_data, global_inputs, targets = get_global_data(json_data)\n",
    "sequential_inputs = get_sequential_data(json_data)\n",
    "\n",
    "# Ordinal encoding of categorical inputs\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}\n",
    "\n",
    "df = to_dataframe(\n",
    "    meta_data=meta_data,\n",
    "    global_inputs=global_inputs,\n",
    "    sequential_inputs=sequential_inputs,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    targets=targets,\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sequential_inputs\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "seed_num = 1337\n",
    "np.random.seed(seed_num)\n",
    "keras.utils.set_random_seed(seed_num)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.05, random_state=seed_num)\n",
    "print(f\"Train Dataframe: {train_df.shape}\")\n",
    "print(f\"Test Dataframe: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training MLPs <a class=\"anchor\" id=\"train-mlps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Data Preprocessing <a class=\"anchor\" id=\"mlp-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = [  # Selecting input features\n",
    "    \"strategy\",\n",
    "    \"board\",\n",
    "    # \"precision\",\n",
    "    \"bit_width\",\n",
    "    # \"integer_bits\",\n",
    "    # \"fractional_bits\",\n",
    "    \"reuse_mean\",\n",
    "    # \"dense_count\",\n",
    "    # \"batchnormalization_count\",\n",
    "    # \"add_count\",\n",
    "    # \"concatenate_count\",\n",
    "    # \"dropout_count\",\n",
    "    # \"relu_count\",\n",
    "    # \"sigmoid_count\",\n",
    "    # \"tanh_count\",\n",
    "    # \"softmax_count\",\n",
    "    # \"dense_parameters_mean\",\n",
    "    # \"dense_inputs_mean\",\n",
    "    # \"dense_outputs_mean\",\n",
    "    # \"dense_reuse_mean\",\n",
    "    \"dense_inputs_mean\",\n",
    "    # \"dense_inputs_min\",\n",
    "    # \"dense_inputs_min_idx\",\n",
    "    # \"dense_inputs_max\",\n",
    "    # \"dense_inputs_max_idx\",\n",
    "    \"dense_outputs_mean\",\n",
    "    # \"dense_outputs_min\",\n",
    "    # \"dense_outputs_min_idx\",\n",
    "    # \"dense_outputs_max\",\n",
    "    # \"dense_outputs_max_idx\",\n",
    "    \"dense_parameters_mean\",\n",
    "    # \"dense_parameters_min\",\n",
    "    # \"dense_parameters_min_idx\",\n",
    "    # \"dense_parameters_max\",\n",
    "    # \"dense_parameters_max_idx\",\n",
    "    \"dense_reuse_mean\",\n",
    "    # \"dense_reuse_min\",\n",
    "    # \"dense_reuse_min_idx\",\n",
    "    # \"dense_reuse_max\",\n",
    "    # \"dense_reuse_max_idx\",\n",
    "    \"dense_count\",\n",
    "    \"batchnormalization_inputs_mean\",\n",
    "    # \"batchnormalization_inputs_min\",\n",
    "    # \"batchnormalization_inputs_min_idx\",\n",
    "    # \"batchnormalization_inputs_max\",\n",
    "    # \"batchnormalization_inputs_max_idx\",\n",
    "    \"batchnormalization_outputs_mean\",\n",
    "    # \"batchnormalization_outputs_min\",\n",
    "    # \"batchnormalization_outputs_min_idx\",\n",
    "    # \"batchnormalization_outputs_max\",\n",
    "    # \"batchnormalization_outputs_max_idx\",\n",
    "    \"batchnormalization_parameters_mean\",\n",
    "    # \"batchnormalization_parameters_min\",\n",
    "    # \"batchnormalization_parameters_min_idx\",\n",
    "    # \"batchnormalization_parameters_max\",\n",
    "    # \"batchnormalization_parameters_max_idx\",\n",
    "    \"batchnormalization_count\",\n",
    "    # \"add_inputs_mean\",\n",
    "    # \"add_inputs_min\",\n",
    "    # \"add_inputs_min_idx\",\n",
    "    # \"add_inputs_max\",\n",
    "    # \"add_inputs_max_idx\",\n",
    "    # \"add_outputs_mean\",\n",
    "    # \"add_outputs_min\",\n",
    "    # \"add_outputs_min_idx\",\n",
    "    # \"add_outputs_max\",\n",
    "    # \"add_outputs_max_idx\",\n",
    "    \"add_count\",\n",
    "    # \"concatenate_inputs_mean\",\n",
    "    # \"concatenate_inputs_min\",\n",
    "    # \"concatenate_inputs_min_idx\",\n",
    "    # \"concatenate_inputs_max\",\n",
    "    # \"concatenate_inputs_max_idx\",\n",
    "    # \"concatenate_outputs_mean\",\n",
    "    # \"concatenate_outputs_min\",\n",
    "    # \"concatenate_outputs_min_idx\",\n",
    "    # \"concatenate_outputs_max\",\n",
    "    # \"concatenate_outputs_max_idx\",\n",
    "    \"concatenate_count\",\n",
    "    # \"dropout_inputs_mean\",\n",
    "    # \"dropout_inputs_min\",\n",
    "    # \"dropout_inputs_min_idx\",\n",
    "    # \"dropout_inputs_max\",\n",
    "    # \"dropout_inputs_max_idx\",\n",
    "    # \"dropout_outputs_mean\",\n",
    "    # \"dropout_outputs_min\",\n",
    "    # \"dropout_outputs_min_idx\",\n",
    "    # \"dropout_outputs_max\",\n",
    "    # \"dropout_outputs_max_idx\",\n",
    "    \"dropout_count\",\n",
    "    # \"relu_inputs_mean\",\n",
    "    # \"relu_inputs_min\",\n",
    "    # \"relu_inputs_min_idx\",\n",
    "    # \"relu_inputs_max\",\n",
    "    # \"relu_inputs_max_idx\",\n",
    "    # \"relu_outputs_mean\",\n",
    "    # \"relu_outputs_min\",\n",
    "    # \"relu_outputs_min_idx\",\n",
    "    # \"relu_outputs_max\",\n",
    "    # \"relu_outputs_max_idx\",\n",
    "    \"relu_count\",\n",
    "    # \"sigmoid_inputs_mean\",\n",
    "    # \"sigmoid_inputs_min\",\n",
    "    # \"sigmoid_inputs_min_idx\",\n",
    "    # \"sigmoid_inputs_max\",\n",
    "    # \"sigmoid_inputs_max_idx\",\n",
    "    # \"sigmoid_outputs_mean\",\n",
    "    # \"sigmoid_outputs_min\",\n",
    "    # \"sigmoid_outputs_min_idx\",\n",
    "    # \"sigmoid_outputs_max\",\n",
    "    # \"sigmoid_outputs_max_idx\",\n",
    "    \"sigmoid_count\",\n",
    "    # \"tanh_inputs_mean\",\n",
    "    # \"tanh_inputs_min\",\n",
    "    # \"tanh_inputs_min_idx\",\n",
    "    # \"tanh_inputs_max\",\n",
    "    # \"tanh_inputs_max_idx\",\n",
    "    # \"tanh_outputs_mean\",\n",
    "    # \"tanh_outputs_min\",\n",
    "    # \"tanh_outputs_min_idx\",\n",
    "    # \"tanh_outputs_max\",\n",
    "    # \"tanh_outputs_max_idx\",\n",
    "    \"tanh_count\",\n",
    "    \"softmax_inputs_mean\",\n",
    "    # \"softmax_inputs_min\",\n",
    "    # \"softmax_inputs_min_idx\",\n",
    "    # \"softmax_inputs_max\",\n",
    "    # \"softmax_inputs_max_idx\",\n",
    "    \"softmax_outputs_mean\",\n",
    "    # \"softmax_outputs_min\",\n",
    "    # \"softmax_outputs_min_idx\",\n",
    "    # \"softmax_outputs_max\",\n",
    "    # \"softmax_outputs_max_idx\",\n",
    "    \"softmax_count\",\n",
    "    # \"total_mult\",\n",
    "    # \"total_add\",\n",
    "    # \"total_logical\",\n",
    "    # \"total_lookup\",\n",
    "]\n",
    "\n",
    "train_inputs_df = train_df[feature_labels].copy()\n",
    "test_inputs_df = test_df[feature_labels].copy()\n",
    "\n",
    "train_inputs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = [\"lut\"]\n",
    "\n",
    "train_targets_df = train_df[target_labels].copy()\n",
    "test_targets_df = test_df[target_labels].copy()\n",
    "\n",
    "train_targets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Building and Training <a class=\"anchor\" id=\"fit-mlps\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import MLPSettings, ModelWrapper\n",
    "\n",
    "input_shape = (None, len(train_inputs_df.columns))\n",
    "output_shape = (None, len(train_targets_df.columns))\n",
    "\n",
    "bram_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[32, 16, 32],\n",
    "    dense_layers=[256, 256, 256, 64, 32, 64, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "dsp_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 32, 32],\n",
    "    dense_layers=[256, 16, 32, 32, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "ff_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 16, 32],\n",
    "    dense_layers=[64, 128, 64, 256, 32],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "lut_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 16, 32, 32],\n",
    "    dense_layers=[64, 128, 128, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "cycles_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[32, 16, 64],\n",
    "    dense_layers=[256, 32, 32, 32, 256, 128, 128, 32, 16, 16, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "\n",
    "mlp_settings = lut_mlp_settings\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "model_wrapper.build_mlp_model(\n",
    "    mlp_settings,\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    categorical_maps=global_categorical_maps,\n",
    "    model_name=f\"{'-'.join([x.upper() for x in target_labels])}_MLP\",\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import TrainSettings\n",
    "from rule4ml.models.metrics import parametric_smape, parametric_r2\n",
    "\n",
    "smape = parametric_smape(0, \"-\".join([x.upper() for x in target_labels]))\n",
    "r2 = parametric_r2(0, \"-\".join([x.upper() for x in target_labels]))\n",
    "\n",
    "bram_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "dsp_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "ff_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "lut_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "cycles_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-3,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "\n",
    "train_settings = cycles_train_settings\n",
    "\n",
    "model_wrapper.build_dataset(\n",
    "    train_inputs_df,\n",
    "    train_targets_df,\n",
    "    train_settings.batch_size,\n",
    "    val_ratio=0.15,\n",
    "    train_repeats=10,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = os.path.join(\"./logs\", f\"{model_wrapper.model.name}_{start_time}\")\n",
    "checkpoint_dir = os.path.join(\"./checkpoints\", f\"{model_wrapper.model.name}_{start_time}\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_file = os.path.join(checkpoint_dir, f\"{'-'.join(target_labels)}_best.weights.h5\")\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq=\"epoch\",\n",
    "    embeddings_freq=1,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_file,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        return lr * np.exp(-0.2)\n",
    "    return lr\n",
    "\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "callbacks = [\n",
    "    tensorboard_callback,\n",
    "    checkpoint_callback,\n",
    "    # lr_callback\n",
    "]\n",
    "\n",
    "fit_history = model_wrapper.fit(train_settings, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training Transformers <a class=\"anchor\" id=\"train-transformers\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Data Preprocessing <a class=\"anchor\" id=\"transformer-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_labels = [\n",
    "    \"strategy\",\n",
    "    \"board\",\n",
    "    \"bit_width\",\n",
    "    \"reuse_mean\",\n",
    "    \"dense_inputs_mean\",\n",
    "    \"dense_outputs_mean\",\n",
    "    \"dense_parameters_mean\",\n",
    "    \"dense_reuse_mean\",\n",
    "    \"dense_count\",\n",
    "    \"batchnormalization_inputs_mean\",\n",
    "    \"batchnormalization_outputs_mean\",\n",
    "    \"batchnormalization_parameters_mean\",\n",
    "    \"batchnormalization_count\",\n",
    "    \"add_count\",\n",
    "    \"concatenate_count\",\n",
    "    \"dropout_count\",\n",
    "    \"relu_count\",\n",
    "    \"sigmoid_count\",\n",
    "    \"tanh_count\",\n",
    "    \"softmax_inputs_mean\",\n",
    "    \"softmax_outputs_mean\",\n",
    "    \"softmax_count\",\n",
    "]\n",
    "sequential_feature_labels = [\n",
    "    \"layer_type\",\n",
    "    \"layer_input_size\",\n",
    "    \"layer_output_size\",\n",
    "    \"layer_parameter_count\",\n",
    "    \"layer_reuse\",\n",
    "]\n",
    "\n",
    "feature_labels = global_feature_labels\n",
    "if len(sequential_feature_labels) > 0:\n",
    "    feature_labels += [\"sequential_inputs\"]\n",
    "inputs_df = df[feature_labels].copy()\n",
    "inputs_df[\"sequential_inputs\"] = inputs_df[\"sequential_inputs\"].apply(\n",
    "    lambda x: x[sequential_feature_labels]\n",
    ")\n",
    "\n",
    "inputs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_df[\"sequential_inputs\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = [\"lut\"]\n",
    "targets_df = df[target_labels].copy()\n",
    "targets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Building and Training <a class=\"anchor\" id=\"fit-transformers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import TransformerSettings, ModelWrapper\n",
    "\n",
    "global_input_shape = (None, len(inputs_df.columns) - 1)  # not considering \"sequential_inputs\"\n",
    "sequential_input_shape = (None, len(inputs_df[\"sequential_inputs\"].iloc[0].columns))\n",
    "output_shape = (None, len(targets_df.columns))\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "model_wrapper.build_transformer_model(\n",
    "    TransformerSettings(\n",
    "        global_dense_layers=[128, 192, 192],\n",
    "        seq_dense_layers=[32, 64, 96],\n",
    "        global_numerical_dense_layers=[16, 8],\n",
    "        seq_numerical_dense_layers=[32],\n",
    "        num_blocks=1,\n",
    "        num_heads=8,\n",
    "        ff_dim=256,\n",
    "        output_dim=192,\n",
    "        dropout_rate=0.2,\n",
    "        embedding_outputs=[24, 24, 16, 8],\n",
    "        dense_layers=[192, 128, 64, 32, 64, 128, 256, 32],\n",
    "        dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    ),\n",
    "    global_input_shape=global_input_shape,\n",
    "    sequential_input_shape=sequential_input_shape,\n",
    "    output_shape=output_shape,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    model_name=f\"{'-'.join([x.upper() for x in target_labels])}_Transformer\",\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import TrainSettings\n",
    "\n",
    "train_settings = TrainSettings(num_epochs=50)\n",
    "model_wrapper.build_dataset(\n",
    "    inputs_df,\n",
    "    targets_df,\n",
    "    train_settings.batch_size,\n",
    "    val_ratio=0.15,\n",
    "    train_repeats=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = os.path.join(\"./logs\", f\"{model_wrapper.model.name}_{start_time}\")\n",
    "checkpoint_dir = os.path.join(\"./checkpoints\", f\"{model_wrapper.model.name}_{start_time}\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_file = os.path.join(checkpoint_dir, f\"{'-'.join(target_labels)}_best.weights.h5\")\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq=\"epoch\",\n",
    "    embeddings_freq=1,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_file,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        return lr * np.exp(-0.2)\n",
    "    return lr\n",
    "\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "callbacks = [\n",
    "    tensorboard_callback,\n",
    "    checkpoint_callback,\n",
    "    # lr_callback\n",
    "]\n",
    "\n",
    "fit_history = model_wrapper.fit(train_settings, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Finetuning (Optional) <a class=\"anchor\" id=\"finetune\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Finetuning an MLP <a class=\"anchor\" id=\"finetune-mlp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.tuning import Searcher\n",
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "target_labels = [\"lut\"]\n",
    "\n",
    "train_targets_df = train_df[target_labels].copy()\n",
    "test_targets_df = test_df[target_labels].copy()\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "searcher = Searcher(model_wrapper)\n",
    "searcher.mlp_search(\n",
    "    train_inputs_df,\n",
    "    train_targets_df,\n",
    "    global_categorical_maps,\n",
    "    directory=\"./mlp_search\",\n",
    "    verbose=1,\n",
    ")\n",
    "searcher.tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Loading and Retraining <a class=\"anchor\" id=\"load-tuner\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.tuning import Searcher\n",
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "searcher = Searcher(model_wrapper)\n",
    "searcher.load_tuner(\n",
    "    train_inputs_df,\n",
    "    train_targets_df,\n",
    "    global_categorical_maps,\n",
    "    \"./mlp_search\",\n",
    "    \"20240715-090815\",\n",
    ")\n",
    "searcher.tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import TrainSettings\n",
    "\n",
    "model_wrapper = searcher.model_wrapper\n",
    "model_wrapper.fit(searcher.train_settings, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing Prediction Models <a class=\"anchor\" id=\"test-models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Benchmark Networks <a class=\"anchor\" id=\"benchmark-test\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Add,\n",
    "    Input,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Flatten,\n",
    ")\n",
    "\n",
    "\n",
    "def get_test_model(name):\n",
    "    model = None\n",
    "    if name == \"jet\":\n",
    "        input_size = 16\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(5, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"quarks\":\n",
    "        input_size = 10\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(1, use_bias=True)(x)\n",
    "        outputs = Activation(\"sigmoid\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"anomaly\":\n",
    "        input_size = 128\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(8, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(4, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(128, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(4, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(128, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"bipc\":\n",
    "        input_size = 36\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(36, use_bias=False)(inputs)\n",
    "\n",
    "        y = Activation(\"relu\")(x)\n",
    "        for i in range(5):\n",
    "            y = Dense(36, use_bias=False)(y)\n",
    "            y = Add()([x, y])\n",
    "            y = Activation(\"relu\")(y)\n",
    "        outputs = y\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"cookie\":\n",
    "        input_size = 512\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(4, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(5, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"mnist\":\n",
    "        input_size = 784\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(16, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(10, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"automlp\":\n",
    "        input_size = 7\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(12, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(16, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(12, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(2, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"particle\":\n",
    "        input_size = 14\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(3, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"custom1\":\n",
    "        input_size = 16\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(64, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(10, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"custom2\":\n",
    "        input_size = 128\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(16, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(64, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(64, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(50, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"custom3\":\n",
    "\n",
    "        def residual_block(x, units):\n",
    "            y = Dense(units)(x)\n",
    "            y = BatchNormalization()(y)\n",
    "            y = Activation(\"relu\")(y)\n",
    "\n",
    "            y = Dense(units)(y)\n",
    "            y = BatchNormalization()(y)\n",
    "\n",
    "            if x.shape[-1] == units:\n",
    "                y = Add()([x, y])\n",
    "            else:\n",
    "                x = Dense(units)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                y = Add()([x, y])\n",
    "\n",
    "            y = Activation(\"relu\")(y)\n",
    "            return y\n",
    "\n",
    "        input_size = 64\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "        x = residual_block(x, units=32)\n",
    "        x = residual_block(x, units=32)\n",
    "\n",
    "        x = Dense(10)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"conv2d-nopool\":\n",
    "        input_size = (16, 16, 1)\n",
    "        inputs = Input(input_size)\n",
    "        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2D(4, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(2, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper, MultiModelEstimator\n",
    "import itertools\n",
    "\n",
    "hls_configs = [\n",
    "    {\n",
    "        \"model\": {\n",
    "            \"precision\": \"ap_fixed<8, 3>\",\n",
    "            \"reuse_factor\": 32,\n",
    "            \"strategy\": strategy,\n",
    "            \"bram_factor\": 1000000000,\n",
    "            \"trace_output\": False,\n",
    "        },\n",
    "        \"clock_period\": 10.0,\n",
    "        \"io_type\": \"io_parallel\",\n",
    "        \"board\": board,\n",
    "    }\n",
    "    for board, strategy in itertools.product([\"pynq-z2\", \"zcu102\"], [\"Latency\", \"Resource\"])\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"jet\",\n",
    "    \"quarks\",\n",
    "    \"anomaly\",\n",
    "    \"bipc\",\n",
    "    \"cookie\",\n",
    "    \"mnist\",\n",
    "    \"automlp\",\n",
    "    \"particle\",\n",
    "    \"custom1\",\n",
    "    \"custom2\",\n",
    "    \"custom3\",\n",
    "]\n",
    "models = [get_test_model(name) for name in model_names]\n",
    "\n",
    "target_labels = [\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\"]\n",
    "\n",
    "estimator = MultiModelEstimator()\n",
    "for label in target_labels:\n",
    "    model_wrapper = ModelWrapper()\n",
    "    model_wrapper.load(\n",
    "        f\"./models/best_{label.upper()}_MLP_config.json\",\n",
    "        f\"./models/best_{label.upper()}_MLP.weights.h5\",\n",
    "    )\n",
    "    estimator.add_model_wrapper(model_wrapper)\n",
    "\n",
    "prediction_df = estimator.predict(models, hls_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.sort_values([\"Board\", \"Strategy\", \"Reuse Factor\"]).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Plots <a class=\"anchor\" id=\"plots\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Box Plots <a class=\"anchor\" id=\"box-plots\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from rule4ml.parsers.data_parser import (\n",
    "    read_from_json,\n",
    "    ParsedDataFilter,\n",
    "    get_global_data,\n",
    "    get_sequential_data,\n",
    "    to_dataframe,\n",
    ")\n",
    "\n",
    "from rule4ml.parsers.data_parser import (\n",
    "    default_board_map,\n",
    "    default_strategy_map,\n",
    "    default_layer_type_map,\n",
    ")\n",
    "\n",
    "data_filter = ParsedDataFilter(\n",
    "    max_output_size=200,\n",
    ")\n",
    "\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "json_data = read_from_json(\n",
    "    os.path.join(base_path, \"datasets/fcnn_dataset_15000.json\"),\n",
    "    data_filter,\n",
    ")\n",
    "\n",
    "meta_data, global_inputs, targets = get_global_data(json_data)\n",
    "sequential_inputs = get_sequential_data(json_data)\n",
    "\n",
    "# Ordinal encoding of categorical inputs\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}\n",
    "\n",
    "df = to_dataframe(\n",
    "    meta_data=meta_data,\n",
    "    global_inputs=global_inputs,\n",
    "    sequential_inputs=sequential_inputs,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    targets=targets,\n",
    ")\n",
    "\n",
    "seed_num = 1337\n",
    "np.random.seed(seed_num)\n",
    "keras.utils.set_random_seed(seed_num)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.05, random_state=seed_num)\n",
    "\n",
    "feature_labels = [\n",
    "    \"strategy\",\n",
    "    \"board\",\n",
    "    \"bit_width\",\n",
    "    \"reuse_mean\",\n",
    "    \"dense_inputs_mean\",\n",
    "    \"dense_inputs_min\",\n",
    "    \"dense_inputs_min_idx\",\n",
    "    \"dense_inputs_max\",\n",
    "    \"dense_inputs_max_idx\",\n",
    "    \"dense_outputs_mean\",\n",
    "    \"dense_outputs_min\",\n",
    "    \"dense_outputs_min_idx\",\n",
    "    \"dense_outputs_max\",\n",
    "    \"dense_outputs_max_idx\",\n",
    "    \"dense_parameters_mean\",\n",
    "    \"dense_parameters_min\",\n",
    "    \"dense_parameters_min_idx\",\n",
    "    \"dense_parameters_max\",\n",
    "    \"dense_parameters_max_idx\",\n",
    "    \"dense_reuse_mean\",\n",
    "    \"dense_reuse_min\",\n",
    "    \"dense_reuse_min_idx\",\n",
    "    \"dense_reuse_max\",\n",
    "    \"dense_reuse_max_idx\",\n",
    "    \"dense_count\",\n",
    "    \"batchnormalization_inputs_mean\",\n",
    "    \"batchnormalization_outputs_mean\",\n",
    "    \"batchnormalization_parameters_mean\",\n",
    "    \"batchnormalization_count\",\n",
    "    \"add_count\",\n",
    "    \"concatenate_count\",\n",
    "    \"dropout_count\",\n",
    "    \"relu_count\",\n",
    "    \"sigmoid_count\",\n",
    "    \"tanh_count\",\n",
    "    \"softmax_inputs_mean\",\n",
    "    \"softmax_outputs_mean\",\n",
    "    \"softmax_count\",\n",
    "]\n",
    "\n",
    "test_inputs_df = test_df[feature_labels].copy()\n",
    "print(f\"Test Inputs: {test_inputs_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "prediction_labels = [\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\"]\n",
    "test_targets_df = test_df[prediction_labels].copy()\n",
    "\n",
    "wrappers = []\n",
    "prediction_errors = []\n",
    "for label in prediction_labels:\n",
    "    wrapper = ModelWrapper()\n",
    "    wrapper.load(\n",
    "        f\"./models/best_{label.upper()}_MLP_config.json\",\n",
    "        f\"./models/best_{label.upper()}_MLP.weights.h5\",\n",
    "    )\n",
    "    wrappers.append(wrapper)\n",
    "\n",
    "    pred = wrapper.predict_from_df(test_inputs_df).squeeze()\n",
    "    gn = test_targets_df[label].values\n",
    "\n",
    "    prediction_errors.append(np.abs(gn - pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(prediction_errors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "fig, axis = plt.subplots(2, 2, figsize=(12, 8), width_ratios=[3, 1])\n",
    "axis = np.reshape(axis, -1)\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.4)\n",
    "\n",
    "flier_ax, box_ax = axis[0], axis[2]\n",
    "\n",
    "iqr_weight = 1.5\n",
    "\n",
    "resources_errors = prediction_errors[:4]\n",
    "resources_labels = prediction_labels[:4]\n",
    "\n",
    "threshold = 10.0\n",
    "below_threshold = []\n",
    "for errors in np.asarray(resources_errors):\n",
    "    below_threshold.append(np.sum(errors < threshold) / len(errors))\n",
    "print(f\"Resources below {threshold}%: {below_threshold}\")\n",
    "print(f\"Resources Mean: {np.mean(below_threshold)}\")\n",
    "\n",
    "bplot = box_ax.boxplot(\n",
    "    resources_errors,\n",
    "    whis=iqr_weight,\n",
    "    tick_labels=[x.upper() for x in resources_labels],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "fliers = flier_ax.boxplot(\n",
    "    resources_errors,\n",
    "    whis=iqr_weight,\n",
    "    tick_labels=[\"\" for x in resources_labels],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "\n",
    "colors = [\"pink\", \"yellow\", \"lightgreen\", \"lightblue\", \"FFA500\"]\n",
    "for patch, color in zip(bplot[\"boxes\"], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "box_ax.set_ylim(-1, 30)\n",
    "flier_ax.set_ylim(30, 200)\n",
    "\n",
    "box_ax.yaxis.grid(True)\n",
    "box_ax.spines.top.set_visible(False)\n",
    "box_ax.xaxis.tick_bottom()\n",
    "box_ax.set_yticks([0, 5, 10, 15, 20, 25, 30])\n",
    "\n",
    "flier_ax.yaxis.grid(True)\n",
    "flier_ax.spines.bottom.set_visible(False)\n",
    "flier_ax.xaxis.tick_top()\n",
    "flier_ax.set_yticks([30, 50, 75, 100, 125, 150, 175, 200])\n",
    "\n",
    "d = 0.5\n",
    "kwargs = dict(\n",
    "    marker=[(-1, -d), (1, d)],\n",
    "    markersize=12,\n",
    "    linestyle=\"none\",\n",
    "    color=\"k\",\n",
    "    mec=\"k\",\n",
    "    mew=1,\n",
    "    clip_on=False,\n",
    ")\n",
    "flier_ax.plot([0, 1], [0, 0], transform=flier_ax.transAxes, **kwargs)\n",
    "box_ax.plot([0, 1], [1, 1], transform=box_ax.transAxes, **kwargs)\n",
    "\n",
    "median_line = Line2D([0], [0], color=\"orange\", linestyle=\"--\", linewidth=1.5, label=\"Median\")\n",
    "mean_line = Line2D([0], [0], color=\"green\", linestyle=\"--\", linewidth=1.5, label=\"Mean\")\n",
    "\n",
    "handles = [median_line, mean_line]\n",
    "labels = [\"Median\", \"Mean\"]\n",
    "\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    bbox_to_anchor=[0.9, 1],\n",
    "    # loc=\"upper left\",\n",
    "    loc=\"upper right\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "ytext = fig.text(0.06, 0.5, \"Error (%)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "suptitle = fig.suptitle(\"Prediction Errors - Boxplots\", fontsize=20, y=0.95)\n",
    "\n",
    "latency_flier_ax, latency_box_ax = axis[1], axis[3]\n",
    "\n",
    "iqr_weight = 1.5\n",
    "\n",
    "latency_errors = [prediction_errors[4]]\n",
    "latency_labels = [prediction_labels[4]]\n",
    "\n",
    "threshold = 100.0\n",
    "below_threshold = []\n",
    "for errors in np.asarray(latency_errors):\n",
    "    below_threshold.append(np.sum(errors < threshold) / len(errors))\n",
    "print(f\"Latency below {threshold} cycles: {below_threshold}\")\n",
    "\n",
    "latency_bplot = latency_box_ax.boxplot(\n",
    "    latency_errors,\n",
    "    whis=iqr_weight,\n",
    "    widths=0.33,\n",
    "    tick_labels=[\"Cycles\"],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "latency_fliers = latency_flier_ax.boxplot(\n",
    "    latency_errors,\n",
    "    whis=iqr_weight,\n",
    "    widths=0.33,\n",
    "    tick_labels=[\"\" for x in latency_labels],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "\n",
    "colors = [\"lightblue\"]\n",
    "for patch, color in zip(latency_bplot[\"boxes\"], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "latency_box_ax.set_ylim(-10, 200)\n",
    "latency_flier_ax.set_ylim(200, 650)\n",
    "\n",
    "latency_box_ax.yaxis.grid(True)\n",
    "latency_box_ax.spines.top.set_visible(False)\n",
    "latency_box_ax.xaxis.tick_bottom()\n",
    "latency_box_ax.set_yticks(np.arange(0, 225, 25))\n",
    "\n",
    "latency_flier_ax.yaxis.grid(True)\n",
    "latency_flier_ax.spines.bottom.set_visible(False)\n",
    "latency_flier_ax.xaxis.tick_top()\n",
    "latency_flier_ax.set_yticks(np.arange(200, 700, 100))\n",
    "\n",
    "d = 0.5\n",
    "kwargs = dict(\n",
    "    marker=[(-1, -d), (1, d)],\n",
    "    markersize=12,\n",
    "    linestyle=\"none\",\n",
    "    color=\"k\",\n",
    "    mec=\"k\",\n",
    "    mew=1,\n",
    "    clip_on=False,\n",
    ")\n",
    "latency_flier_ax.plot([0, 1], [0, 0], transform=latency_flier_ax.transAxes, **kwargs)\n",
    "latency_box_ax.plot([0, 1], [1, 1], transform=latency_box_ax.transAxes, **kwargs)\n",
    "\n",
    "latency_ytext = fig.text(0.66, 0.5, \"Error (Cycles)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "resource_caption = fig.text(0.355, 0.04, \"(a)\", va=\"center\", size=18)\n",
    "latency_caption = fig.text(0.808, 0.04, \"(b)\", va=\"center\", size=18)\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/box_plot_merged.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, ytext, suptitle, latency_ytext, resource_caption, latency_caption),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Bar Plots <a class=\"anchor\" id=\"bar-plots\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper, MultiModelEstimator\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "prediction_labels = [\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\"]\n",
    "\n",
    "model_names = [\n",
    "    \"jet\",\n",
    "    \"quarks\",\n",
    "    \"anomaly\",\n",
    "    \"bipc\",\n",
    "    \"cookie\",\n",
    "    \"mnist\",\n",
    "    \"automlp\",\n",
    "    \"particle\",\n",
    "    \"custom1\",\n",
    "    \"custom2\",\n",
    "    \"custom3\",\n",
    "]\n",
    "test_models = [get_test_model(name) for name in model_names]\n",
    "\n",
    "hls_configs = [\n",
    "    {\n",
    "        \"model\": {\n",
    "            \"precision\": precision,\n",
    "            \"reuse_factor\": reuse,\n",
    "            \"strategy\": strategy,\n",
    "            \"bram_factor\": 1000000000,\n",
    "            \"trace_output\": False,\n",
    "        },\n",
    "        \"clock_period\": 10.0,\n",
    "        \"io_type\": \"io_parallel\",\n",
    "        \"board\": board,\n",
    "    }\n",
    "    for board, strategy, precision, reuse in itertools.product(\n",
    "        [\"pynq-z2\", \"zcu102\"],\n",
    "        [\"Latency\", \"Resource\"],\n",
    "        [\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\", \"ap_fixed<16, 6>\"],\n",
    "        [1, 2, 4, 8, 16, 32, 64],\n",
    "    )\n",
    "]\n",
    "\n",
    "estimator = MultiModelEstimator()\n",
    "estimator.load_default_models()\n",
    "predictions = []\n",
    "\n",
    "prediction_df = estimator.predict(test_models, hls_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[\"BRAM\"] = prediction_df[\"BRAM\"].apply(lambda x: min(x, 200.0))\n",
    "prediction_df[\"DSP\"] = prediction_df[\"DSP\"].apply(lambda x: min(x, 200.0))\n",
    "prediction_df[\"FF\"] = prediction_df[\"FF\"].apply(lambda x: min(x, 200.0))\n",
    "prediction_df[\"LUT\"] = prediction_df[\"LUT\"].apply(lambda x: min(x, 200.0))\n",
    "\n",
    "precision_order = [\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\", \"ap_fixed<16, 6>\"]\n",
    "prediction_df[\"Precision\"] = pd.Categorical(\n",
    "    prediction_df[\"Precision\"], categories=precision_order, ordered=True\n",
    ")\n",
    "\n",
    "prediction_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.parsers.data_parser import (\n",
    "    read_from_json,\n",
    "    get_global_data,\n",
    "    get_sequential_data,\n",
    "    to_dataframe,\n",
    "    default_strategy_map,\n",
    "    default_board_map,\n",
    "    default_layer_type_map,\n",
    ")\n",
    "\n",
    "benchmark_data = read_from_json(\"../datasets/benchmark_data.json\")\n",
    "\n",
    "benchmark_meta_data, benchmark_global_inputs, benchmark_targets = get_global_data(benchmark_data)\n",
    "benchmark_sequential_inputs = get_sequential_data(benchmark_data)\n",
    "\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}\n",
    "\n",
    "benchmark_df = to_dataframe(\n",
    "    meta_data=benchmark_meta_data,\n",
    "    global_inputs=benchmark_global_inputs,\n",
    "    sequential_inputs=benchmark_sequential_inputs,\n",
    "    global_categorical_maps={},\n",
    "    sequential_categorical_maps={},\n",
    "    targets=benchmark_targets,\n",
    ")\n",
    "benchmark_gn_df = benchmark_df[\n",
    "    [\n",
    "        \"model_name\",\n",
    "        \"board\",\n",
    "        \"strategy\",\n",
    "        \"precision\",\n",
    "        \"global_reuse\",\n",
    "        \"bram\",\n",
    "        \"dsp\",\n",
    "        \"ff\",\n",
    "        \"lut\",\n",
    "        \"cycles\",\n",
    "    ]\n",
    "].copy()\n",
    "benchmark_gn_df = benchmark_gn_df.rename(\n",
    "    {\n",
    "        \"model_name\": \"Model\",\n",
    "        \"board\": \"Board\",\n",
    "        \"strategy\": \"Strategy\",\n",
    "        \"precision\": \"Precision\",\n",
    "        \"global_reuse\": \"Reuse Factor\",\n",
    "        \"bram\": \"BRAM\",\n",
    "        \"dsp\": \"DSP\",\n",
    "        \"ff\": \"FF\",\n",
    "        \"lut\": \"LUT\",\n",
    "        \"cycles\": \"CYCLES\",\n",
    "    },\n",
    "    axis=1,\n",
    ")\n",
    "benchmark_gn_df.loc[benchmark_gn_df[\"Strategy\"] == \"latency\", \"Strategy\"] = \"Latency\"\n",
    "benchmark_gn_df.loc[benchmark_gn_df[\"Strategy\"] == \"resource\", \"Strategy\"] = \"Resource\"\n",
    "\n",
    "benchmark_gn_df[\"BRAM\"] = benchmark_gn_df[\"BRAM\"].apply(lambda x: min(x, 200.0))\n",
    "benchmark_gn_df[\"DSP\"] = benchmark_gn_df[\"DSP\"].apply(lambda x: min(x, 200.0))\n",
    "benchmark_gn_df[\"FF\"] = benchmark_gn_df[\"FF\"].apply(lambda x: min(x, 200.0))\n",
    "benchmark_gn_df[\"LUT\"] = benchmark_gn_df[\"LUT\"].apply(lambda x: min(x, 200.0))\n",
    "\n",
    "precision_order = [\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\", \"ap_fixed<16, 6>\"]\n",
    "benchmark_gn_df[\"Precision\"] = pd.Categorical(\n",
    "    benchmark_gn_df[\"Precision\"], categories=precision_order, ordered=True\n",
    ")\n",
    "\n",
    "benchmark_gn_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_grouped_mean = (\n",
    "    benchmark_gn_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[\n",
    "        [\n",
    "            \"BRAM\",\n",
    "            \"DSP\",\n",
    "            \"FF\",\n",
    "            \"LUT\",\n",
    "            # \"CYCLES\"\n",
    "        ]\n",
    "    ]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "prediction_grouped_mean = (\n",
    "    prediction_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[\n",
    "        [\n",
    "            \"BRAM\",\n",
    "            \"DSP\",\n",
    "            \"FF\",\n",
    "            \"LUT\",\n",
    "            # \"CYCLES\"\n",
    "        ]\n",
    "    ]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    gn_grouped_mean,\n",
    "    prediction_grouped_mean,\n",
    "    on=(\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"),\n",
    "    suffixes=(\" (G)\", \" (P)\"),\n",
    ")\n",
    "\n",
    "merged_df = merged_df[\n",
    "    [\n",
    "        \"BRAM (G)\",\n",
    "        \"BRAM (P)\",\n",
    "        \"DSP (G)\",\n",
    "        \"DSP (P)\",\n",
    "        \"FF (G)\",\n",
    "        \"FF (P)\",\n",
    "        \"LUT (G)\",\n",
    "        \"LUT (P)\",\n",
    "        # \"CYCLES (G)\", \"CYCLES (P)\",\n",
    "    ]\n",
    "]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rule4ml.parsers.utils import fixed_precision_to_bit_width\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "grouped = merged_df.xs((\"pynq-z2\",), level=[\"Board\"]).groupby([\"Precision\", \"Strategy\"])\n",
    "\n",
    "n_groups = len(grouped)\n",
    "n_cols = 2\n",
    "n_rows = 3\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, dpi=300, figsize=(16, 10), squeeze=False, sharex=True, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "width = 0.11\n",
    "colors = [\"#008000\", \"#FF5964\", \"#17BEBB\", \"#FFA500\"]\n",
    "reuse_factors = prediction_df[\"Reuse Factor\"].unique()\n",
    "num_resources = 4\n",
    "resource_gap = 0\n",
    "\n",
    "total_width = num_resources * (2 * width + resource_gap) - resource_gap\n",
    "start = np.arange(1, len(reuse_factors) + 1) - total_width / 2\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for ax, ((precision, strategy), df) in zip(axes, grouped):\n",
    "    for i, (col_gn, col_pred) in enumerate(zip(df.columns[::2], df.columns[1::2])):\n",
    "        gn_vals = df[col_gn]\n",
    "        pred_vals = df[col_pred]\n",
    "\n",
    "        resource_indices = start + i * (2 * width + resource_gap)\n",
    "\n",
    "        for j, reuse_factor in enumerate(reuse_factors):\n",
    "            gn_label = \"\"\n",
    "            pred_label = \"\"\n",
    "            if j == 0:\n",
    "                gn_label = f\"{col_gn}\"\n",
    "                pred_label = f\"{col_pred}\"\n",
    "\n",
    "            ax.bar(\n",
    "                resource_indices[j] - width / 2,\n",
    "                gn_vals[j],\n",
    "                width,\n",
    "                label=gn_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            ax.bar(\n",
    "                resource_indices[j] + width / 2,\n",
    "                pred_vals[j],\n",
    "                width,\n",
    "                label=pred_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "                hatch=\"///\",\n",
    "            )\n",
    "\n",
    "    total_bits, fraction_bits = fixed_precision_to_bit_width(precision)\n",
    "\n",
    "    ax.set_title(f\"{strategy}, {total_bits}-bit width\")\n",
    "    ax.set_xticks(start + (num_resources - 1) * (width + resource_gap / 2))\n",
    "    ax.set_xticklabels(reuse_factors, rotation=45)\n",
    "\n",
    "    # if col_idx == 0:\n",
    "    #     ax.set_ylabel(\"Utilization (%)\")\n",
    "\n",
    "    # if row_idx == n_rows - 1:\n",
    "    #     ax.set_xlabel(\"Reuse Factor\")\n",
    "\n",
    "    col_idx += 1\n",
    "    if col_idx == n_cols:\n",
    "        row_idx += 1\n",
    "        col_idx = 0\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Resources\",\n",
    "    bbox_to_anchor=[0.3, 1.03],\n",
    "    loc=\"upper left\",\n",
    "    # loc=\"upper right\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "xtext = fig.text(0.5, 0.035, \"Reuse Factor\", ha=\"center\", size=18)\n",
    "ytext = fig.text(0.07, 0.5, \"Utilization (%)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "suptitle = fig.suptitle(\"Pynq-Z2: Resource Utilization Trends\", fontsize=20, y=1.075)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.275, wspace=0.125)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/pynq_avg_bars.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, xtext, ytext, suptitle),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "grouped = merged_df.xs((\"zcu102\",), level=[\"Board\"]).groupby([\"Precision\", \"Strategy\"])\n",
    "\n",
    "n_groups = len(grouped)\n",
    "n_cols = 2\n",
    "n_rows = 3\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, dpi=300, figsize=(16, 10), squeeze=False, sharex=True, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "width = 0.11\n",
    "colors = [\"#008000\", \"#FF5964\", \"#17BEBB\", \"#FFA500\"]\n",
    "reuse_factors = prediction_df[\"Reuse Factor\"].unique()\n",
    "num_resources = 4\n",
    "resource_gap = 0\n",
    "\n",
    "total_width = num_resources * (2 * width + resource_gap) - resource_gap\n",
    "start = np.arange(1, len(reuse_factors) + 1) - total_width / 2\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for ax, ((precision, strategy), df) in zip(axes, grouped):\n",
    "    for i, (col_gn, col_pred) in enumerate(zip(df.columns[::2], df.columns[1::2])):\n",
    "        gn_vals = df[col_gn]\n",
    "        pred_vals = df[col_pred]\n",
    "\n",
    "        resource_indices = start + i * (2 * width + resource_gap)\n",
    "\n",
    "        for j, reuse_factor in enumerate(reuse_factors):\n",
    "            gn_label = \"\"\n",
    "            pred_label = \"\"\n",
    "            if j == 0:\n",
    "                gn_label = f\"{col_gn}\"\n",
    "                pred_label = f\"{col_pred}\"\n",
    "\n",
    "            ax.bar(\n",
    "                resource_indices[j] - width / 2,\n",
    "                gn_vals[j],\n",
    "                width,\n",
    "                label=gn_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            ax.bar(\n",
    "                resource_indices[j] + width / 2,\n",
    "                pred_vals[j],\n",
    "                width,\n",
    "                label=pred_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "                hatch=\"///\",\n",
    "            )\n",
    "\n",
    "    total_bits, fraction_bits = fixed_precision_to_bit_width(precision)\n",
    "\n",
    "    ax.set_title(f\"{strategy}, {total_bits}-bit width\")\n",
    "    ax.set_xticks(start + (num_resources - 1) * (width + resource_gap / 2))\n",
    "    ax.set_xticklabels(reuse_factors, rotation=45)\n",
    "\n",
    "    col_idx += 1\n",
    "    if col_idx == n_cols:\n",
    "        row_idx += 1\n",
    "        col_idx = 0\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Resources\",\n",
    "    bbox_to_anchor=[0.3, 1.03],\n",
    "    loc=\"upper left\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "xtext = fig.text(0.5, 0.035, \"Reuse Factor\", ha=\"center\", size=18)\n",
    "ytext = fig.text(0.07, 0.5, \"Utilization (%)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "suptitle = fig.suptitle(\"ZCU102: Resource Utilization Trends\", fontsize=20, y=1.075)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.275, wspace=0.125)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/zcu_avg_bars.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, xtext, ytext, suptitle),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_grouped_mean = (\n",
    "    benchmark_gn_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[[\"CYCLES\"]]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "prediction_grouped_mean = (\n",
    "    prediction_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[[\"CYCLES\"]]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    gn_grouped_mean,\n",
    "    prediction_grouped_mean,\n",
    "    on=(\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"),\n",
    "    suffixes=(\" (G)\", \" (P)\"),\n",
    ")\n",
    "\n",
    "merged_df = merged_df[\n",
    "    [\n",
    "        \"CYCLES (G)\",\n",
    "        \"CYCLES (P)\",\n",
    "    ]\n",
    "]\n",
    "merged_df.head()\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "grouped = merged_df.groupby([\"Board\", \"Strategy\", \"Precision\"])\n",
    "\n",
    "n_groups = len(grouped)\n",
    "n_cols = 3\n",
    "n_rows = (n_groups // n_cols) + (n_groups % n_cols > 0)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, dpi=300, figsize=(16, 10), squeeze=False, sharex=True, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "width = 0.35\n",
    "colors = [\"#17BEBB\", \"#FFA500\"]\n",
    "reuse_factors = prediction_df[\"Reuse Factor\"].unique()\n",
    "num_resources = 1\n",
    "resource_gap = 0\n",
    "\n",
    "total_width = num_resources * (2 * width + resource_gap) - resource_gap\n",
    "start = np.arange(1, len(reuse_factors) + 1) - total_width / 2\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for ax, ((board, strategy, precision), df) in zip(axes, grouped):\n",
    "    for i, (col_gn, col_pred) in enumerate(zip(df.columns[::2], df.columns[1::2])):\n",
    "        gn_vals = df[col_gn]\n",
    "        pred_vals = df[col_pred]\n",
    "\n",
    "        resource_indices = start + i * (2 * width + resource_gap)\n",
    "\n",
    "        for j, reuse_factor in enumerate(reuse_factors):\n",
    "            gn_label = \"\"\n",
    "            pred_label = \"\"\n",
    "            if j == 0:\n",
    "                gn_label = f\"{col_gn}\"\n",
    "                pred_label = f\"{col_pred}\"\n",
    "\n",
    "            ax.bar(\n",
    "                resource_indices[j] - width / 2,\n",
    "                gn_vals[j],\n",
    "                width,\n",
    "                label=gn_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            ax.bar(\n",
    "                resource_indices[j] + width / 2,\n",
    "                pred_vals[j],\n",
    "                width,\n",
    "                label=pred_label,\n",
    "                color=colors[i % len(colors) + 1],\n",
    "                edgecolor=\"black\",\n",
    "                hatch=\"///\",\n",
    "            )\n",
    "\n",
    "    total_bits, fraction_bits = fixed_precision_to_bit_width(precision)\n",
    "\n",
    "    ax.set_title(f\"{board}, {strategy}, {total_bits}-bit width\")\n",
    "    ax.set_xticks(start + (num_resources - 1) * (width + resource_gap / 2))\n",
    "    ax.set_xticklabels(reuse_factors, rotation=45)\n",
    "\n",
    "    col_idx += 1\n",
    "    if col_idx == n_cols:\n",
    "        row_idx += 1\n",
    "        col_idx = 0\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legends = fig.legend(handles, labels, bbox_to_anchor=[0.8, 1], loc=\"upper left\")\n",
    "\n",
    "xtext = fig.text(0.5, 0.05, \"Reuse Factor\", ha=\"center\", size=18)\n",
    "ytext = fig.text(0.07, 0.5, \"Cycles\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "fig.suptitle(\"Clock Cycle Trends\", fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/cycles_avg_bars.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, xtext, ytext),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
